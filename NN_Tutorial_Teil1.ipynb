{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurale Netzwerke - Tutorial Teil 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem zweiten Teil wollen wir mittels spezieller Pythonbibliotheken den Aufbau eines solchen Netzwerkes vereinfachen. Hierfür bedienen wir uns der **PyTorch**-Bibliothek (<http://pytorch.org/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurz vorweg:    \n",
    "_Tieferführende Literaturvorschläge_:\n",
    "* Weitere Tutorials zu PyTorch: <http://pytorch.org/tutorials/>\n",
    "* Sehr gute Einführung in neuronale Netzwerke: <http://neuralnetworksanddeeplearning.com/>\n",
    "* Das absolute Topbuch zu Deep Learning allgemein: <http://www.deeplearningbook.org/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Installation sollte vergleichsweise einfach ablaufen (wenn Python 3.6, siehe Einführung in Python, über Miniconda oder Anaconda installiert wurde).\n",
    "Je nach Betriebssystem müssen in der Kommandozeile innerhalb unserer virtuellen Umgebung folgende Installationsbefehle eingegeben werden:\n",
    "\n",
    "_Installieren der relevanten Python-Bibliotheken_:   \n",
    "`conda install pytorch=0.4.1 cuda80 -c pytorch`   \n",
    "`conda install torchvision -c pytorch`      \n",
    "\n",
    "\n",
    "Hier gehe ich davon aus das erstmal niemand direkten Zugriff auf eine Nvidia-GPU hat. Falls je, in den Kommentaren nach `[Mit GPU]` schauen.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aufbau des Netzwerkes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 FCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Teil des Tutorials haben wir ein einfaches **Fully-connected Neural Network (FCN)** aufgebaut, mit 3 Schichten zu 748, 30 und 10 Neuronen. Das wollen wir mit PyTorch replizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zusammenfassung:\n",
      " Sequential(\n",
      "  (0): Linear(in_features=784, out_features=30, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=30, out_features=10, bias=True)\n",
      "  (3): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "### PyTorch importieren\n",
    "import torch\n",
    "\n",
    "FCN = torch.nn.Sequential(\n",
    "      torch.nn.Linear(784, 30), # Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\n",
    "      torch.nn.ReLU(),          # Aktivierungsfunktion\n",
    "      torch.nn.Linear(30, 10),  # Zweite/Erste Schicht: 30 \"versteckte\" Neuronen gehen über zu 10 Ausgangsneuronen.\n",
    "      torch.nn.Softmax(dim=1)   # Ausgangsaktivierungsfunktion. Siehe vorheriges Tutorial.\n",
    "      )\n",
    "\n",
    "print(\"Zusammenfassung:\\n\",FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und das wars! Das gesamte Netzwerk ist fertig aufgebaut und bereit zum Verwenden!\n",
    "\n",
    "Zum genaueren Verständnis gehen wir aber alles noch einmal stückweise durch:\n",
    "Zuerst importieren wir die PyTorch-Bibliothek mit \n",
    "\n",
    "`import torch`\n",
    "\n",
    "Danach rufen wir das Submodul `nn` und dessen Unterklasse `Sequential` auf. `Sequential()` ist ein Hülle, in welche wir unsere Schichten reinstecken, die das Netzwerk ausmachen. Das kann man sich wie einen Binder vorstellen, welcher die einzelnen Papiere zusammen hält. Wir im Binder ist die Reihenfolge wichtig!\n",
    "\n",
    "In unserem Beispiel sind die Schichten die einzelnen Neuronschichten `nn.Linear()`. \n",
    "\n",
    "Diese nehmen als Input die eingehenden und ausgehenden Neuronen. Wer sich im vorherigen Tutorial an die Matrizen-Notation erinnert, der weiß, dass das nötig ist, um die Transformationsmatrix zu konstruieren, die beispielsweise von der Eingangsschicht zur Versteckten transformiert.\n",
    "\n",
    "Für die Extraportion Nichtlinearität setzen wir wie schon im vorherigen Tutorial Aktivierungsfunktionen ein (`nn.ReLU()`), sowie `nn.Softmax()` als Ausgabefunktion, damit wir Wahrscheinlichkeiten rausgegeben bekommen.\n",
    "\n",
    "Und voilà - das Netzwerk ist komplett. Um es nun zu trainieren, müssen wir alles Drumherum wieder aufsetzen: Das Einlesen der Daten sowie die Präparation und im Anschluss noch einige Eigenheiten von PyTorch beachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Gesamte Trainingsstruktur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben dem einfachen Aufbau von Netzen bietet PyTorch zum Einlesen und Verwenden der Daten noch viele hilfreiche Mittel! Entsprechend werde ich zuerst die Struktur zeigen, dann Stück für Stück erläutern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MNIST_Data_Provider(Dataset):\n",
    "    def __init__(self, all_image_paths, all_image_labels):\n",
    "        super(MNIST_Data_Provider, self).__init__()\n",
    "        #Dieser befehl gibt in Kurzfassung an, das Grundeinstellungen\n",
    "        #aus der Vaterklasse geerbt werden sollen, in diesem Fall\n",
    "        #torch.utils.data.Dataset.\n",
    "        \n",
    "        self.all_image_paths, self.all_image_labels = all_image_paths, all_image_labels\n",
    "        # Alle Dateipfade setzen\n",
    "        \n",
    "        self.transform_to_torch_tensor = transforms.ToTensor()\n",
    "        # PyTorch arbeitet mit Tensor-Objekten, analog wie Numpy mit\n",
    "        # numpy-array Objekten arbeitet.\n",
    "        \n",
    "        self.hot_list = np.eye(10).astype(int)     \n",
    "        \n",
    "    def one_hot(self, label):\n",
    "        \"\"\"\n",
    "        Zahl zu Vektornotation konvertieren.\n",
    "        \"\"\"\n",
    "        return self.hot_list[label]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        loaded_image    = Image.open(self.all_image_paths[idx])\n",
    "        label_for_image = self.all_image_labels[idx]\n",
    "        \n",
    "        return self.transform_to_torch_tensor(loaded_image), label_for_image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "    \n",
    "\n",
    "def get_image_paths(path_to_folder):\n",
    "    all_image_paths = []\n",
    "    all_labels      = []\n",
    "    for numberpath in os.listdir(path_to_folder):\n",
    "        if numberpath != \".DS_Store\":\n",
    "            all_image_paths.extend([path_to_folder+\"/\"+numberpath+\"/\"+x for x in os.listdir(path_to_folder+\"/\"+numberpath)])\n",
    "            all_labels.extend([int(numberpath) for _ in range(len(os.listdir(path_to_folder+\"/\"+numberpath)))])\n",
    "    return all_image_paths, all_labels    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier erstellen wir eine Datenklasse, welche von `torch.utils.data.Dataset` erbt (die genauen Spezifikationen sind nicht so wichtig). Ihre Aufgabe ist es, `datagen()` aus dem vorherigen Tutorial zu ersetzen und uns die Bilder zurückzugeben. \n",
    "Wie das Bild zurückgegeben wird ist in der Funktion `__getitem__()` festgesetzt, welche auf Anfrage ein Trainingsbild zurückgibt.\n",
    "\n",
    "\n",
    "Damit lässt sich das Training des Netzes nun wie folgt gestalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\"\"\" Wichtige Hyperparameter setzen \"\"\"\n",
    "# Hyperparameter nennt man Parameter, die \n",
    "# indirekt die Perfomance unseres Netzwerkes beeinflussen.\n",
    "batch_size             = 64\n",
    "n_epochs               = 10\n",
    "learning_rate          = 0.003\n",
    "train_validation_split = 0.8 #Prozentuale Einteilung in Trainings- und Validierungsdaten\n",
    "\n",
    "# Pfad zum Trainingsset. In diesem Fall befindet sich dieses im gleichen Ordner wie\n",
    "# das Jupyter Notebook.\n",
    "path_to_MNIST_dataset = os.getcwd()+\"/trainingSet\"\n",
    "\n",
    "# Wie im vorherigen Tutorial holen wir uns hier eine Liste aller Bildpfade und Labels.\n",
    "all_image_paths, all_image_labels = get_image_paths(path_to_MNIST_dataset)\n",
    "\n",
    "# Hier werden diese dann durchmischt. \n",
    "# np.random.seed() sorgt dafür, das stets die gleiche Durchmischung stattfindet.\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_paths)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_labels)\n",
    "\n",
    "#Aufteilungsindex zwischen Trainings- und Validierungsdatensatz.\n",
    "split_idx = int(len(all_image_paths)*train_validation_split)\n",
    "\n",
    "#Aufbau des Trainings-Datengenerators\n",
    "training_img_paths = all_image_paths[:split_idx]\n",
    "training_labels    = all_image_labels[:split_idx]\n",
    "train_dataset = MNIST_Data_Provider(training_img_paths, training_labels)\n",
    "train_datagen = DataLoader(train_dataset, batch_size=batch_size,drop_last=True, shuffle=True, num_workers=0)\n",
    "\n",
    "#Aufbau des Validierungsdatengenerators\n",
    "validation_img_paths = all_image_paths[split_idx:]\n",
    "validation_labels    = all_image_labels[split_idx:]\n",
    "val_dataset = MNIST_Data_Provider(validation_img_paths, validation_labels)\n",
    "val_datagen = DataLoader(val_dataset, batch_size=batch_size, drop_last=True, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufbau des Netzwerkes wie oben.\n",
    "FCN = torch.nn.Sequential(\n",
    "      torch.nn.Linear(784, 150), # Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\n",
    "      torch.nn.LeakyReLU(),          # Aktivierungsfunktion\n",
    "      torch.nn.Linear(150, 30),  # Zweite/Erste Schicht: 30 \"versteckte\" Neuronen gehen über zu 10 Ausgangsneuronen.\n",
    "      torch.nn.LeakyReLU(),          # Aktivierungsfunktion\n",
    "      torch.nn.Linear(30, 10),  # Zweite/Erste Schicht: 30 \"versteckte\" Neuronen gehen über zu 10 Ausgangsneuronen.    \n",
    "      torch.nn.Softmax(dim=1)   # Ausgangsaktivierungsfunktion. Siehe vorheriges Tutorial.\n",
    "      )\n",
    "\n",
    "\n",
    "#Sammler verschiedener Metriken, in diesem Fall der Trainings\n",
    "#und Validierungsgenauigkeiten\n",
    "data_coll = {\"t_acc\":[], \"v_acc\":[]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das obige Skript beinhaltet alles, was nötig ist, um ein Neuronales Netzwerk zu trainieren: WIr haben unseren Datengenerator `datagen()` (mehr dazu gleich), unser **FCN**, eine Kostenfunktion `F.cross_entropy()`(siehe vorheriges Tutorial) sowie als Optimierungsalgorithmus einen Standard-Gradientenminimierer, `optim.SGD()`, wobei _SGD_ kurz für _Stochastic Gradient Descent_ ist.\n",
    "\n",
    "**Dataloader**\n",
    "Der `Dataloader` selbst ist eine Hülle für unser `Dataset` welches wir weiteroben geschrieben haben. Die Aufgabe hier ist es einfach, das Rausziehen der Daten zu vereinfachen und zu parallelisieren. Die Parameter, die man ihm übergibt, sind dabei die `batch_size`, d.h. für wie viele Bilder der Gradient berechnet werden soll, bis die Netzwerkgewichte upgedatet werden, `num_workers`, welches der Anzahl der CPU-Kernen entspricht sowie `shuffle` und `remove_last`, welches für Durchmischen der Daten und entfernen von Batches sind, die nicht `batch_size` entsprechen.\n",
    "\n",
    "**Optimierer**\n",
    "Als Optimierer haben wir hier einen _Stochastic Gradient Descent_-Algorithmus verwendet, welcher auch im vorherigen Tutorial zu finden ist. Alternativen, die zum Teil wesentlich besser funktionieren, finden sich unter <http://pytorch.org/docs/master/nn.html#loss-functions>.\n",
    "Eine genauere Betrachtung verschiedener Optimierer werden wir, wenn die Zeit da ist, noch durchgehen. Ansonsten einfach ausprobieren!\n",
    "\n",
    "Generell lassen sich diese aber in zwei Klassen einteilen: Adaptive und Nicht-adaptive Methoden, welche sich darin unterscheiden, ob die Lernrate angepasst (_adaptiv_) oder belassen wird.\n",
    "\n",
    "In die zweite Kategorie fallen Methoden wie `SGD`, `SGD mit Impuls` oder `Adadelta`, wobei letztere Methoden eine zerfallende Summe vergangener Gradienten sammeln, um schneller konvergieren zu können und Sattelpunkten bzw. kleineren lokalen Minima entkommen können:\n",
    "\n",
    "Erinnern wir uns an die Vorschrift für `SGD`:\n",
    "\n",
    "-  Für die Gewichte: \n",
    "\\begin{equation}\n",
    "w_{ik}^{j(t+1)} = w_{ik}^{jt} - \\frac{\\eta}{bs} \\sum_{s=1}^{bs}\\frac{\\partial L(\\textbf{o}_s,\\textbf{t}_s)}{\\partial w_{ik}^{jt}}\n",
    "\\end{equation}\n",
    "-  Für die Offsets:\n",
    "\\begin{equation}\n",
    "b_{i}^{j(t+1)} = b_{i}^{jt} - \\frac{\\eta}{bs} \\sum_{s=1}^{bs}\\frac{\\partial L(\\textbf{o}_s,\\textbf{t}_s)}{\\partial b_{i}^{jt}}\n",
    "\\end{equation}\n",
    "\n",
    "so sieht beispielsweise `SGD mit Impuls`, oder auch `SGD with Momentum` so aus (gezeigt anhand der Gewichte):\n",
    "\n",
    "\\begin{equation}\n",
    "w_{ik}^{j(t+1)} = w_{ik}^{jt} - \\eta \\cdot \\nu^{jt}_{ik}\n",
    "\\end{equation}\n",
    "\n",
    "wobei \n",
    "\n",
    "\\begin{equation}\n",
    "\\nu^{jt}_{ik} = \\gamma \\cdot \\nu^{j(t-1)}_{ik} + \\frac{1}{bs} \\sum_{s=1}^{bs}\\frac{\\partial L(\\textbf{o}_s,\\textbf{t}_s)}{\\partial w_{ik}^{jt}}\n",
    "\\end{equation}\n",
    "\n",
    "Der Parameter $\\gamma\\in [0,1]$ wird als \"Reibung\" bezeichnet, da er die momentane Summe der Gradienten, analog zu einer Geschwindigkeit reduziert. Wohlgemerkt für $\\gamma=0$ haben wir einfach `SGD`.\n",
    "\n",
    "Adaptive Methoden verwenden ähnliche Kollektionen, d.h. zerfallende Summen der Gradienten im ersten und/oder zweiten Momenten, um die Lernrate in sinnvolle Richtungen zu lenken.\n",
    "\n",
    "Darunter fallen beispielsweise `Adam`, `RMSPROP` oder `Nadam`.\n",
    "Für mehr Info, siehe neben der offiziellen PyTorch Dokumentation, zum Beispiel http://ruder.io/optimizing-gradient-descent/.\n",
    "\n",
    "---\n",
    "\n",
    "Andere wichtige **Schlüsselwörter** sind im Code selbst erklärt!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Epoch 0...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02564 | T-Acc 83.2083% | V-Acc 93.0582%\n",
      "Training in Epoch 1...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02380 | T-Acc 94.0774% | V-Acc 94.1198%\n",
      "Training in Epoch 2...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02360 | T-Acc 95.1667% | V-Acc 95.2409%\n",
      "Training in Epoch 3...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02349 | T-Acc 95.8869% | V-Acc 94.9905%\n",
      "Training in Epoch 4...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02342 | T-Acc 96.2917% | V-Acc 94.6565%\n",
      "Training in Epoch 5...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02337 | T-Acc 96.5833% | V-Acc 95.5153%\n",
      "Training in Epoch 6...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02332 | T-Acc 96.8363% | V-Acc 95.9924%\n",
      "Training in Epoch 7...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02326 | T-Acc 97.3155% | V-Acc 96.1236%\n",
      "Training in Epoch 8...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02325 | T-Acc 97.3304% | V-Acc 95.6942%\n",
      "Training in Epoch 9...\n",
      "\t T-Progress: [301/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02326 | T-Acc 97.2292% | V-Acc 95.9924%\n"
     ]
    }
   ],
   "source": [
    "#Start des eigentlichen Trainings.\n",
    "device = torch.device('cpu')\n",
    "# ### Wenn eine GPU verwendet wird, einfach diese Zeile ausklammern:\n",
    "# device = torch.device('cuda')\n",
    "_ = FCN.to(device)\n",
    "\n",
    "#Optimierungsalgorithmus - Er nimmt die Lernrate und die Netzwerkparameter und updated diese je\n",
    "#nach Gradienten. Diese wird später im Traininglauf berechnet (unten).\n",
    "optimizer = optim.Adam(FCN.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Training in Epoch {}...\".format(epoch))\n",
    "    \n",
    "    \"\"\" Hier startet das Training! \"\"\"\n",
    "    FCN.train() #Schlüsselwort um das Netz in Trainingsmodus zu stecken.\n",
    "                #Hier für uns nicht direkt wichtig, aber einige besondere Netzwerkschichten\n",
    "                #verhalten sich während Training und Validierung unterschiedlich\n",
    "    \n",
    "    train_avg_loss = 0\n",
    "    train_avg_acc  = 0\n",
    "    \n",
    "    for idx, (img,label) in enumerate(train_datagen):\n",
    "        \n",
    "        # Unser Datengenerator gibt uns Bilder und Labels aus.\n",
    "        # Diese müssen wir für PyTorch in Objekte umwandeln, für die\n",
    "        # mathematisch ein Gradient berechnet werden kann. \n",
    "        # Ausserdem schieben wir unser Objekt nach <device>.\n",
    "        # Dies ist relevant, falls <device> eine GPU bei uns stellt.\n",
    "        img, label = img.to(device), label.to(device)\n",
    "\n",
    "        # Die Vorhersage unseres Netzwerkes für ein Satz Bilddaten, welche vorher zu Form\n",
    "        # (Batch_size, n_classes) vektorisiert wird.\n",
    "        output = FCN(img.view(batch_size,-1))\n",
    "        \n",
    "        # Unsere Kostenfunktion, Categorical Cross-Entropy.\n",
    "        loss = F.cross_entropy(output, label)\n",
    "\n",
    "                \n",
    "        # Hier setzen wir vormals berechnete Gradienten of 0.\n",
    "        optimizer.zero_grad()\n",
    "        # Dann berechnen wir diese erneut abhängig zur Kostenfunktion.\n",
    "        loss.backward()\n",
    "        # Im Anschluss werden alle Gewichte upgedatet.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Anzahl an korrekt vorhergesagter Bilder.\n",
    "        # Erklärung des Kettenbegriffs:\n",
    "        # Mit Variable()-Objekten kann nicht gut direkt gearbeitet werden. Daher nehmen wir uns deren\n",
    "        # Zahleninhalt mit .data, suchen das entsprechende Maximum entlang der 1. Achse und vergleichen das\n",
    "        # elementweise mit dem wahren Wert über .eq(label.data). Im Anschluss wird das Ergebnis aufsummiert und gibt\n",
    "        # die Anzahl korrekter Vorhersagen pro Batch zurück.\n",
    "        correct_guesses = output.cpu().detach().max(1)[1].eq(label.cpu().detach()).sum()\n",
    "        \n",
    "        # Hier sammeln wir Kosten und Genauigkeit während dem Training.\n",
    "        train_avg_loss += loss.item()\n",
    "        train_avg_acc  += correct_guesses.numpy()\n",
    "        \n",
    "        if idx%300==0 and idx!=0:\n",
    "            print(\"\\t T-Progress: [{}/{}]\\r\".format(idx+1,len(train_datagen)))\n",
    "\n",
    "    # Um prozentuale Ergebnisse zu bekommen, mitteln wir alles durch die Anzahl an Trainingsbildern.\n",
    "    train_avg_loss = train_avg_loss*1./(batch_size*len(train_datagen))\n",
    "    train_avg_acc  = train_avg_acc*1./(batch_size*len(train_datagen))\n",
    "    \n",
    "    data_coll[\"t_acc\"].append(train_avg_acc)\n",
    "    \"\"\" Hier startet die Validierung \"\"\"\n",
    "    FCN.eval() #Netzwerk wird in Validierungs-/Testmodus gesteckt.\n",
    "    \n",
    "    ### Ab hier das gleiche Prozeder wie während dem Training, lediglich ohne \n",
    "    ### Trainieren der Parameter.\n",
    "    val_avg_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (img,label) in enumerate(val_datagen):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            output     = FCN(img.view(batch_size,-1))\n",
    "            correct_guesses = output.cpu().detach().max(1)[1].eq(label.cpu().detach()).sum()\n",
    "\n",
    "            val_avg_acc  += correct_guesses.numpy()\n",
    "\n",
    "            if idx%100==0 and idx!=0:\n",
    "                print(\"\\t V-Progress: [{}/{}]\\r\".format(idx+1,len(val_datagen)))    \n",
    "\n",
    "    val_avg_acc = val_avg_acc*1./(batch_size*len(val_datagen))\n",
    "    data_coll[\"v_acc\"].append(val_avg_acc)\n",
    "    print(\"Results: T-Loss {0:2.5f} | T-Acc {1:3.4f}% | V-Acc {2:3.4f}%\".format(train_avg_loss, train_avg_acc*100., val_avg_acc*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Aufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geht durch die Beschreibungen auf <http://pytorch.org/docs/master/nn.html> und schaut, ob ihr Änderungen findet, die das Netz evtl. besser trainieren lassen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aufgabe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nehmt einen Trainingsdurchgang auf und plottet Kosten/Genauigkeit gegen Epoche oder Iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data_coll[\"t_acc\"],\"b\",label='Training Accuracy')\n",
    "plt.plot(data_coll[\"v_acc\"],\"r\",label='Validation Accuracy')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis hierhin haben wir uns mit **FCN**s auseinandergesetzt, welche versuchen (zumindest in stark vereinfachenden Zügen) das Gehirn und seine Neuronen nachzustellen. Eine zweite Klasse, die solche Netzwerke aus einem rein mathematischen Standpunkt angeht, sind sog. **Convolutional Neural Network**s **(CNN)**s.\n",
    "\n",
    "Anders als bei unseren FCNs haben wir es hier mit **Filtern** statt Neuronen zu tun. Zudem nehmen sie als Eingangssignal direkt Bilder oder andere volumetrische Daten. Die Grobstruktur sieht aus wie in der unteren Abbildung (Quelle: >https://www.kdnuggets.com/2015/11/understanding-convolutional-neural-networks-nlp.html>).\n",
    "\n",
    "<img src=\"Bilder/cnn.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine ergänzende Darstellung (entnommen von <https://stackoverflow.com/questions/42733971/convolutional-layer-to-fully-connected-layer-in-cnn>) findet sich ebenso:\n",
    "\n",
    "<img src=\"Bilder/cnn2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beide Bilder (ansonsten einfach das Internet nach Bildern durchforsten) reichen hoffentlich aus, um die Wirkungsweise solche Netzwerke klarzustellen.\n",
    "\n",
    "Wir starten mit unserem Bild (in unserem Fall das Bild einer Zahl) und füttern es in ein **CNN**. Dieses hat, ähnlich wie das **FCN**, eine Reihe von Schichten:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist die Hauptschicht (ähnlich den Neuronen im FCN). Eine Hauptschicht ist eine Ansammlung sog. Filter, kleine Matrizen mit Gewichten, welche jeweils über das Bild geschoben werden und eine Convolution/Faltung berechnen (mathematisch eigentlich eine Korrelation, aber das tut nichts zur Sache). Das sieht etwa so aus:\n",
    "\n",
    "<img src=\"Bilder/conv_pic_example.png\">\n",
    "\n",
    "Wie man erkennen kann, nimmt ein Filter eine Pixelgruppe und gewichtet die Werte zu *einem* Aktivierungswert. Macht er das für alle Pixelgruppen auf einem Bild, entsteht eine sog. **Feature map** - eine Abbildung des Bildes zu einem Bild welches besondere Bildeigenschaften hervorhebt, z.B. Kanten.\n",
    "\n",
    "Da sich in jeder Schicht mehrere solcher Filter befinden, gibt es auch entsprechend viele solcher **Feature maps**. In obigen Bild ist dies durch die Breite (_96, 256, ..._) angegeben, da am Ende einer Schicht alle dieser feature maps verkettet werden, denn:\n",
    "\n",
    "In der nächsten Schicht nehmen sich die Filter wieder Pixelgruppen vor, dieses mal aber für alle Pixel in allen feature maps innerhalb einer Gruppenposition.\n",
    "\n",
    "**Ein Beispiel:**\n",
    "Seien in der ersten Schicht 20 Filter vorhanden. Das Eingangsbild hat die Maße $224\\times 224$. Die Filter haben je die Größe $3\\times3$. Nach Anwenden verbleiben wir mit 20 $222\\times222$-Bildern (die reduzierte Größe liegt daran, das an den Bildkanten der Filter etwas verschoben werden muss und so die Randpixel nicht gleich oft wie innere Pixel erfasst werden). Nach Verkettung macht das einen $20\\times222\\times222$-Klotz.\n",
    "In der nächsten Schicht haben dann die Filter die Ausmaße $20\\times3\\times3$, da die Informationen aus **allen** feature maps zusammengeführt werden!\n",
    "\n",
    "**Anmerkung**: Um die Bildverkleinerung zu bekämpfen, kann man stets das Eingangsbild/die Eingangsfeaturemaps an den Seiten mit Nullen ausstatten, und die Größe vor den Berechnungen etwas erhöhen, sodas am Ende wieder die gleiche Größe herauskommt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Anzahl an Gewichten zu reduzieren, führt man ganz gerne **Pooling Layer** ein. Das sind Minischichten, deren einzige Aufgabe es ist, die Ausmaße des Netzes zu reduzieren. Sie definieren sich über eine **Kerngröße/Kernelsize** ähnlich den Filtern oben. Sie nehmen sich eine Pixelgruppe vor und führen eine Operation aus, z.B. den Maximalwert oder das arithmetische Mittel nehmen. Entsprechend nennt man solche Schichten auch **Maxpooling**- bzw. **Averagepooling**-**Layer**.\n",
    "\n",
    "**Ein Beispiel**:\n",
    "Ein Satz Featuremaps der Größe $20\\times200\\times200$ hat nach einem *Maxpoolinglayer* mit *Kernelsize* $2\\times2$ die Größe $20\\times100\\times100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Sonstige Schichten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ansonsten sind die restlichen Schichten sehr ähnlich zu denen in einem **FCN**, d.h. eine Schicht mit einer Aktivierungsfunktion und/oder andere Funktionen.\n",
    "\n",
    "Zudem ist es nicht unüblich, das **CNN** mit einem kleinen **FCN** abzuschließen. Hierfür werden die letzten Featuremaps entsprechend vektorisiert (d.h. bei einem finalen Featureklotz der Größe $20\\times15\\times15$ wird ein Vektor der Länge $4500$ ausgespuckt) und in ein **FCN** gesteckt.\n",
    "\n",
    "Das ist auch in den obigen Bildern zu sehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir uns wieder an die Implementierung machen und schauen, wie sich diese Netzwerkstruktur im Vergleich zu einem **FCN** schlägt. Natürlich verwenden wir wieder PyTorch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Für das Netzwerk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitiv würde man meinen, dass man die Notation von oben verwenden, um das Netz zu implementieren, d.h etwas in diser Richtung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "CNN = torch.nn.Sequential(\n",
    "        nn.Conv2d(1,10, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(10,20, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)), #Hier fehlt der Übergang von Bild- zu Vektorstruktur    \n",
    "        nn.Linear(320,50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50,10),\n",
    "        nn.Softmax(dim=1)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allerdings kann so nur umständlich der Übergang von Bild- zu Vektorstruktur erreicht werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daher werden wir hier noch eine andere PyTorch-Notation einführen, mit der man kompliziertere Netze aufbauen kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN_Base(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Base,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)  # 26x26\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3) # 24x24\n",
    "        self.conv3 = nn.Conv2d(20, 30, kernel_size=3,padding=1)  # 12x12\n",
    "        self.conv4 = nn.Conv2d(30, 30, kernel_size=3,padding=1)  # 12x12\n",
    "        self.conv5 = nn.Conv2d(30, 50, kernel_size=3,padding=1) # 6x6        \n",
    "        self.conv6 = nn.Conv2d(50, 50, kernel_size=3,padding=1)# 6x6                \n",
    "        self.fc1 = nn.Linear(1800, 150)\n",
    "        self.fc2 = nn.Linear(150, 50)\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.out_act = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv2(self.conv1(x)), 2))\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv4(self.conv3(x)), 2))\n",
    "        x = F.leaky_relu(self.conv6(self.conv5(x)), 2)\n",
    "        x = x.view(-1, 1800)   #Hier ändern wir Bild- zu Vektorformat\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))        \n",
    "        x = self.fc3(x)\n",
    "        return self.out_act(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist alles aufgeteilt in zwei Teile:\n",
    "Innerhalb der `__init__()`-Funktion werden alle Netzschichten initialisiert, die Gewichte besitzen (d.h. mit zufälligen Gewichten ausgestattet, ähnlich einen Lexikon verfügbarer Bauklötze.\n",
    "\n",
    "Die tatsächliche Verknüpfung findet dann in `forward()` statt, d.h. hier wird dann mit den im Lexikon zu findenden Bauklötzen das eigentliche Netzwerk aufgebaut.\n",
    "\n",
    "Das erlaubt es, sehr komplizierte Strukturen zu implementieren. Die Funktionsweise und die Verwendung im allgemeinen Programm bleibt aber die Gleiche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Für das allgemeine Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir nur das Netzwerk abgeändert haben, können wir einfach die **FCN**-Trainingsstruktur kopieren und einfach unser neues Netzwerk einfügen. Die einzigen Änderungen die zu beachten sind ist die Tatsache, dass die Bilder nun nicht mehr Vektoren, sondern einfach Bilder sind. Dafür müssen wir aber lediglich die `img.view(batch_size,-1)`-Zeile entfernen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\"\"\" Hyperparameter \"\"\"\n",
    "batch_size = 64\n",
    "n_epochs   = 20\n",
    "learning_rate = 0.0003\n",
    "train_validation_split = 0.8\n",
    "\n",
    "\"\"\" Aufsetzen der Datengeneratoren \"\"\"\n",
    "path_to_MNIST_dataset = os.getcwd()+\"/MNIST/trainingSet\"\n",
    "\n",
    "all_image_paths, all_image_labels = get_image_paths(path_to_MNIST_dataset)\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_paths)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_labels)\n",
    "\n",
    "split_idx = int(len(all_image_paths)*train_validation_split)\n",
    "\n",
    "training_img_paths = all_image_paths[:split_idx]\n",
    "training_labels    = all_image_labels[:split_idx]\n",
    "train_dataset = MNIST_Data_Provider(training_img_paths, training_labels)\n",
    "train_datagen = DataLoader(train_dataset, batch_size=batch_size,drop_last=True, shuffle=True, num_workers=1)\n",
    "\n",
    "validation_img_paths = all_image_paths[split_idx:]\n",
    "validation_labels    = all_image_labels[split_idx:]\n",
    "val_dataset = MNIST_Data_Provider(validation_img_paths, validation_labels)\n",
    "val_datagen = DataLoader(val_dataset, batch_size=batch_size, drop_last=True, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Epoch 0...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02719 | T-Acc 72.2232% | V-Acc 82.9676%\n",
      "Training in Epoch 1...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02491 | T-Acc 86.6905% | V-Acc 94.0840%\n",
      "Training in Epoch 2...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02357 | T-Acc 95.2738% | V-Acc 96.1594%\n",
      "Training in Epoch 3...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02340 | T-Acc 96.3661% | V-Acc 96.9704%\n",
      "Training in Epoch 4...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02331 | T-Acc 96.9315% | V-Acc 97.2328%\n",
      "Training in Epoch 5...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02328 | T-Acc 97.1458% | V-Acc 97.8531%\n",
      "Training in Epoch 6...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02324 | T-Acc 97.4315% | V-Acc 97.1374%\n",
      "Training in Epoch 7...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02321 | T-Acc 97.5804% | V-Acc 97.6026%\n",
      "Training in Epoch 8...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02321 | T-Acc 97.5357% | V-Acc 97.8650%\n",
      "Training in Epoch 9...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02319 | T-Acc 97.7202% | V-Acc 97.5668%\n",
      "Training in Epoch 10...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02318 | T-Acc 97.7321% | V-Acc 97.9843%\n",
      "Training in Epoch 11...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02318 | T-Acc 97.7827% | V-Acc 97.9604%\n",
      "Training in Epoch 12...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n",
      "\t T-Progress: [301/525]\n",
      "\t T-Progress: [401/525]\n",
      "\t T-Progress: [501/525]\n",
      "\t V-Progress: [101/131]\n",
      "Results: T-Loss 0.02315 | T-Acc 97.9911% | V-Acc 97.2328%\n",
      "Training in Epoch 13...\n",
      "\t T-Progress: [101/525]\n",
      "\t T-Progress: [201/525]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-37:\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/karsten_dl/software/miniconda3/envs/D4L/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/karsten_dl/software/miniconda3/envs/D4L/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-d59d9b59b25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mcorrect_guesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtrain_avg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/karsten_dl/software/miniconda3/envs/D4L/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/karsten_dl/software/miniconda3/envs/D4L/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-7-eddb99f54dee>\", line 38, in __getitem__\n",
      "    loaded_image    = Image.open(self.all_image_paths[idx])\n",
      "  File \"/home/karsten_dl/software/miniconda3/envs/D4L/lib/python3.6/site-packages/PIL/Image.py\", line 2618, in open\n",
      "    prefix = fp.read(16)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Laden des Netzes, Aufsetzen des Optimierers\"\"\"\n",
    "CNN = CNN_Base()\n",
    "device = torch.device('cpu')\n",
    "# ### Falls eine GPU verwendet wird:\n",
    "# device = torch.device('cuda')\n",
    "_ = CNN.to(device)\n",
    "#optimizer = optim.SGD(FCN.parameters(), lr=learning_rate)\n",
    "optimizer = optim.Adam(CNN.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\"\"\" Training & Validierung \"\"\"\n",
    "for epoch in range(n_epochs):\n",
    "    print(\"Training in Epoch {}...\".format(epoch))\n",
    "    \n",
    "    \"\"\" Hier startet das Training! \"\"\"\n",
    "    #FCN.train()\n",
    "    CNN.train()\n",
    "    \n",
    "    train_avg_loss = 0\n",
    "    train_avg_acc  = 0\n",
    "    \n",
    "    data_coll = {\"t_acc\":[], \"v_acc\":[]}\n",
    "    for idx, (img,label) in enumerate(train_datagen):\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #output = FCN(img.view(batch_size,-1))\n",
    "        output = CNN(img)\n",
    "        \n",
    "        loss = F.cross_entropy(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct_guesses = output.cpu().detach().max(1)[1].eq(label.cpu().detach()).sum()\n",
    "        \n",
    "        train_avg_loss += loss.data.item()\n",
    "        train_avg_acc  += correct_guesses.numpy()\n",
    "        \n",
    "        if idx%100==0 and idx!=0:\n",
    "            print(\"\\t T-Progress: [{}/{}]\\r\".format(idx+1,len(train_datagen)))\n",
    "\n",
    "    train_avg_loss = train_avg_loss*1./(batch_size*len(train_datagen))\n",
    "    train_avg_acc  = train_avg_acc*1./(batch_size*len(train_datagen))\n",
    "    \n",
    "    data_coll[\"t_acc\"].append(train_avg_acc)\n",
    "    \n",
    "    \n",
    "    \"\"\" Hier startet die Validierung \"\"\"\n",
    "    #FCN.eval()\n",
    "    CNN.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_avg_acc = 0\n",
    "\n",
    "        for idx, (img,label) in enumerate(val_datagen):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            #output = FCN(img.view(batch_size,-1))\n",
    "            output = CNN(img)\n",
    "            correct_guesses = output.cpu().detach().max(1)[1].eq(label.cpu().detach()).sum()\n",
    "\n",
    "            val_avg_acc  += correct_guesses.numpy()\n",
    "\n",
    "            if idx%100==0 and idx!=0:\n",
    "                print(\"\\t V-Progress: [{}/{}]\\r\".format(idx+1,len(val_datagen)))    \n",
    "\n",
    "        val_avg_acc = val_avg_acc*1./(batch_size*len(val_datagen))\n",
    "        data_coll[\"v_acc\"].append(val_avg_acc)    \n",
    "        print(\"Results: T-Loss {0:2.5f} | T-Acc {1:3.4f}% | V-Acc {2:3.4f}%\".format(train_avg_loss, train_avg_acc*100., val_avg_acc*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit haben wir nun ein weiteres Werkzeug eingeführt, um Bilderkennung zu implementieren.\n",
    "\n",
    "Generell ist es so, dass Netzwerk-Strukturen je nach Bedarf und Zweck entsprechend abgeändert werden. Generell gilt aber: Je tiefer das Netzwerk, desto ausdrucksstärker ist es, aber desto länger, komplizierter und anspruchsvoller ist es, dieses zu sinnvollen Werten trainiert zu bekommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Aufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisier auch hier das Trainingverhalten. Vergleiche dies mit der **FCN**-Struktur von oben. Idealerweise werden beide Trainingskurven in ein Diagramm geplottet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ändere etwas an der Struktur oder dem Optimierungsalgorithmus ab um höhere Ergebnisse erzielt zu bekommen. Wie immer ist die PyTorch-Homepage dein Freund!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleiche die Anzahl der Paramter zwischen **FCN** und **CNN**. Verwende dazu folgenden Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_num_parameters(net):\n",
    "    net_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "    num_params = sum([np.prod(p.size()) for p in net_parameters])\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Damit haben wie die Grundlagen zu neuronalen Netzen abgeschlossen!**\n",
    "\n",
    "Im nächsten Einführungsteil wollen wir uns noch mit Methoden auseinandersetzen, die das Generalisierungsverhalten und das allgemeine Training zu verbessern. Darauffolgend implementieren wir ein Netzwerk um Leber aus CT-Bildern zu segmentieren!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "biomathe",
   "language": "python",
   "name": "biomathe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
