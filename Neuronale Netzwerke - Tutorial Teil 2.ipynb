{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurale Netzwerke - Tutorial Teil 2\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem zweiten Teil wollen wir mittels spezieller Pythonbibliotheken den Aufbau eines solchen Netzwerkes vereinfachen. Hierfür bedienen wir uns der **PyTorch**-Bibliothek (<http://pytorch.org/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurz vorweg:    \n",
    "_Tieferführende Literaturvorschläge_:\n",
    "* Weitere Tutorials zu PyTorch: <http://pytorch.org/tutorials/>\n",
    "* Sehr gute Einführung in neuronale Netzwerke: <http://neuralnetworksanddeeplearning.com/>\n",
    "* Das absolute Topbuch zu Deep Learning allgemein: <http://www.deeplearningbook.org/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Installation sollte vergleichsweise einfach ablaufen (wenn Anaconda mit Python installiert wurde).\n",
    "Je nach Betriebssystem müssen in der Kommandozeile entsprechende Installationsbefehle eingegeben werden:\n",
    "\n",
    "**Für Windows Nutzer:**   \n",
    "_conda install -c peterjc123 pytorch_\n",
    "\n",
    "**Für Mac OSX Nutzer:**   \n",
    "_conda install pytorch torchvision -c pytorch_\n",
    "\n",
    "**Für Linux Nutzer:**   \n",
    "_conda install pytorch-cpu torchvision -c pytorch_\n",
    "\n",
    "Hier gehe ich davon aus das erstmal niemand direkten Zugriff auf eine Nvidia-GPU hat. Falls doch, so ändern sich die Befehle etwas. Das findet sich aber auf der Homepage von PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aufbau des Netzwerkes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 FCNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Teil des Tutorials haben wir ein einfaches **Fully-connected Neural Network (FCN)** aufgebaut, mit 3 Schichten zu 748, 30 und 10 Neuronen. Das wollen wir mit PyTorch replizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-00e811819ac1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m FCN = torch.nn.Sequential(\n\u001b[0;32m      4\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;31m# Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m          \u001b[1;31m# Aktivierungsfunktion\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "FCN = torch.nn.Sequential(\n",
    "      torch.nn.Linear(784, 70), # Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\n",
    "      torch.nn.ReLU(),          # Aktivierungsfunktion\n",
    "      torch.nn.Linear(70, 30), # Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\n",
    "      torch.nn.ReLU(),              \n",
    "      torch.nn.Linear(30, 10),  # Zweite/Erste Schicht: 30 \"versteckte\" Neuronen gehen über zu 10 Ausgangsneuronen.\n",
    "      torch.nn.Softmax(dim=1)   # Ausgangsaktivierungsfunktion. Siehe vorheriges Tutorial.\n",
    "      )\n",
    "\n",
    "print( \"Zusammenfassung:\\n\",FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und das wars! Das gesamte Netzwerk ist fertig aufgebaut und bereit zum Verwenden!\n",
    "\n",
    "Zum genaueren Verständnis gehen wir aber alles noch einmal stückweise durch:\n",
    "Zuerst importieren wir die PyTorch-Bibliothek mit \n",
    "\n",
    "`import torch`\n",
    "\n",
    "Danach rufen wir das Submodul `nn` und dessen Unterklasse `Sequential` auf. `Sequential()` ist ein Hülle, in welche wir unsere Schichten reinstecken, die das Netzwerk ausmachen. Das kann man sich wie einen Binder vorstellen, welcher die einzelnen Papiere zusammen hält. Wir im Binder ist die Reihenfolge wichtig!\n",
    "\n",
    "In unserem Beispiel sind die Schichten die einzelnen Neuronschichten `nn.Linear()`. \n",
    "\n",
    "Diese nehmen als Input die eingehenden und ausgehenden Neuronen. Wer sich im vorherigen Tutorial an die Matrizen-Notation erinnert, der weiß, dass das nötig ist, um die Transformationsmatrix zu konstruieren, die beispielsweise von der Eingangsschicht zur Versteckten transformiert.\n",
    "\n",
    "Für die Extraportion Nichtlinearität setzen wir wie schon im vorherigen Tutorial Aktivierungsfunktionen ein (`nn.ReLU()`), sowie `nn.Softmax()` als Ausgabefunktion, damit wir Wahrscheinlichkeiten rausgegeben bekommen.\n",
    "\n",
    "Und voilà - das Netzwerk ist komplett. Um es nun zu trainieren, müssen wir alles Drumherum wieder aufsetzen: Das Einlesen der Daten sowie die Präparation und im Anschluss noch einige Eigenheiten von PyTorch beachten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Gesamte Trainingsstruktur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben dem einfachen Aufbau von Netzen bietet PyTorch zum Einlesen und Verwenden der Daten noch viele hilfreiche Mittel! Entsprechend werde ich zuerst die Struktur zeigen, dann Stück für Stück erläutern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class MNIST_Data_Provider(Dataset):\n",
    "    def __init__(self, all_image_paths, all_image_labels):\n",
    "        super(MNIST_Data_Provider, self).__init__()\n",
    "\n",
    "        self.all_image_paths, self.all_image_labels = all_image_paths, all_image_labels\n",
    "        self.transform_to_torch_tensor = transforms.ToTensor()\n",
    "        self.hot_list = np.eye(10).astype(int)     \n",
    "        \n",
    "    def one_hot(self, label):\n",
    "        return self.hot_list[label]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        loaded_image    = Image.open(self.all_image_paths[idx])\n",
    "        label_for_image = self.all_image_labels[idx]\n",
    "        \n",
    "        return self.transform_to_torch_tensor(loaded_image), label_for_image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.all_image_paths)\n",
    "    \n",
    "\n",
    "def get_image_paths(path_to_folder):\n",
    "    all_image_paths = []\n",
    "    all_labels      = []\n",
    "    for numberpath in os.listdir(path_to_folder):\n",
    "        if numberpath != \".DS_Store\":\n",
    "            all_image_paths.extend([path_to_folder+\"/\"+numberpath+\"/\"+x for x in os.listdir(path_to_folder+\"/\"+numberpath)])\n",
    "            all_labels.extend([int(numberpath) for _ in range(len(os.listdir(path_to_folder+\"/\"+numberpath)))])\n",
    "    return all_image_paths, all_labels    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier erstellen wir eine Datenklasse, welche von `torch.utils.data.Dataset` erbt (die genauen Spezifikationen sind nicht so wichtig). Ihre Aufgabe ist es, `datagen()` aus dem vorherigen Tutorial zu ersetzen und uns die Bilder zurückzugeben. Damit lässt sich das Training des Netzes nun wie folgt gestalten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\"\"\" Wichtige Hyperparameter\"\"\"\n",
    "# Hyperparameter nennt man Parameter, die indirekt die Perfomance unseres Netzwerkes beeinflussen.\n",
    "batch_size = 32\n",
    "n_epochs   = 10\n",
    "learning_rate = 0.3\n",
    "train_validation_split = 0.8\n",
    "\n",
    "# Pfad zum Trainingsset. In diesem Fall befindet sich dieses im gleichen Ordner wie\n",
    "# das Jupyter Notebook.\n",
    "path_to_MNIST_dataset = \"trainingSet\"\n",
    "\n",
    "# Wie im vorherigen Tutorial holen wir uns hier eine Liste aller Bildpfade und Labels.\n",
    "all_image_paths, all_image_labels = get_image_paths(path_to_MNIST_dataset)\n",
    "\n",
    "# Hier werden diese dann durchmischt. \n",
    "# np.random.seed() sorgt dafür, das stets die gleiche Durchmischung stattfindet.\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_paths)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_labels)\n",
    "\n",
    "#Aufteilungsindex zwischen Trainings- und Validierungsdatensatz.\n",
    "split_idx = int(len(all_image_paths)*train_validation_split)\n",
    "\n",
    "#Aufbau des Trainings-Datengenerators\n",
    "training_img_paths = all_image_paths[:split_idx]\n",
    "training_labels    = all_image_labels[:split_idx]\n",
    "train_dataset = MNIST_Data_Provider(training_img_paths, training_labels)\n",
    "train_datagen = DataLoader(train_dataset, batch_size=batch_size,drop_last=True, shuffle=True, num_workers=0)\n",
    "\n",
    "#Aufbau des Validierungsdatengenerators\n",
    "validation_img_paths = all_image_paths[split_idx:]\n",
    "validation_labels    = all_image_labels[split_idx:]\n",
    "val_dataset = MNIST_Data_Provider(validation_img_paths, validation_labels)\n",
    "val_datagen = DataLoader(val_dataset, batch_size=batch_size, drop_last=True, shuffle=False, num_workers=0)\n",
    "\n",
    "# Aufbau des Netzwerkes wie oben.\n",
    "FCN = torch.nn.Sequential(\n",
    "      torch.nn.Linear(784, 700), # Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\n",
    "      torch.nn.LeakyReLU(),          # Aktivierungsfunktion\n",
    "      torch.nn.Linear(700, 100), # Erste/Zweite Schicht: 784 Eingangsneuronen zu 30 \"versteckten\" Neuronen\n",
    "      torch.nn.LeakyReLU(),          # Aktivierungsfunktion \n",
    "      torch.nn.Linear(100, 30),# Zweite/Erste Schicht: 30 \"versteckte\" Neuronen gehen über zu 10 Ausgangsneuronen.\n",
    "      torch.nn.Softmax(dim=1)   # Ausgangsaktivierungsfunktion. Siehe vorheriges Tutorial.\n",
    "      )\n",
    "\n",
    "#Optimierungsalgorithmus - Er nimmt die Lernrate und die Netzwerkparameter und updated diese je\n",
    "#nach Gradienten. Diese wird später im Traininglauf berechnet (unten).\n",
    "optimizer = optim.SGD(FCN.parameters(), lr=learning_rate)\n",
    "T_Acc_Data=[]\n",
    "V_Acc_Data=[]\n",
    "#Start des eigentlichen Trainings.\n",
    "for epoch in range(n_epochs):\n",
    "    print( \"Training in Epoch {}...\".format(epoch))\n",
    "    \n",
    "    \"\"\" Hier startet das Training! \"\"\"\n",
    "    FCN.train() #Schlüsselwort um das Netz in Trainingsmodus zu stecken.\n",
    "                #Hier für uns nicht direkt wichtig, aber einige besondere Netzwerkschichten\n",
    "                #verhalten sich während Training und Validierung unterschiedlich\n",
    "    \n",
    "    train_avg_loss = 0\n",
    "    train_avg_acc  = 0\n",
    "    \n",
    "    for idx, (img,label) in enumerate(train_datagen):\n",
    "        \n",
    "        # Unser Datengenerator gibt uns Bilder und Labels aus.\n",
    "        # Diese müssen wir für PyTorch in Objekte umwandeln, für die\n",
    "        # mathematisch ein Gradient berechnet werden kann. Dafür\n",
    "        # stecken wir diese in den Variable()-Wrapper.\n",
    "        img, label = Variable(img), Variable(label)\n",
    "\n",
    "        # Die Vorhersage unseres Netzwerkes für ein Satz Bilddaten, welche vorher zu Form\n",
    "        # (Batch_size, n_classes) vektorisiert wird.\n",
    "        output = FCN(img.view(batch_size,-1))\n",
    "        \n",
    "        # Unsere Kostenfunktion, Categorical Cross-Entropy.\n",
    "        loss = F.cross_entropy(output, label)\n",
    "\n",
    "                \n",
    "        # Hier setzen wir vormals berechnete Gradienten of 0.\n",
    "        optimizer.zero_grad()\n",
    "        # Dann berechnen wir diese erneut abhängig zur Kostenfunktion.\n",
    "        loss.backward()\n",
    "        # Im Anschluss werden alle Gewichte upgedatet.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Anzahl an korrekt vorhergesagter Bilder.\n",
    "        # Erklärung des Kettenbegriffs:\n",
    "        # Mit Variable()-Objekten kann nicht gut direkt gearbeitet werden. Daher nehmen wir uns deren\n",
    "        # Zahleninhalt mit .data, suchen das entsprechende Maximum entlang der 1. Achse und vergleichen das\n",
    "        # elementweise mit dem wahren Wert über .eq(label.data). Im Anschluss wird das Ergebnis aufsummiert und gibt\n",
    "        # die Anzahl korrekter Vorhersagen pro Batch zurück.\n",
    "        correct_guesses = output.data.max(1)[1].eq(label.data).sum()\n",
    "        \n",
    "        # Hier sammeln wir Kosten und Genauigkeit während dem Training.\n",
    "        train_avg_loss += loss.data[0]\n",
    "        train_avg_acc  += correct_guesses\n",
    "        \n",
    "        if idx%300==0 and idx!=0:\n",
    "            print (\"\\t T-Progress: [{}/{}]\\r\".format(idx+1,len(train_datagen)))\n",
    "\n",
    "    # Um prozentuale Ergebnisse zu bekommen, mitteln wir alles durch die Anzahl an Trainingsbildern.\n",
    "    train_avg_loss = train_avg_loss*1./(batch_size*len(train_datagen))\n",
    "    train_avg_acc  = train_avg_acc*1./(batch_size*len(train_datagen))\n",
    "    \n",
    "    \"\"\" Hier startet die Validierung \"\"\"\n",
    "    FCN.eval() #Netzwerk wird in Validierungs-/Testmodus gesteckt.\n",
    "    \n",
    "    ### Ab hier das gleiche Prozeder wie während dem Training, lediglich ohne \n",
    "    ### Trainieren der Parameter.\n",
    "    val_avg_acc = 0\n",
    "    \n",
    "    for idx, (img,label) in enumerate(val_datagen):\n",
    "        img, label = Variable(img), Variable(label)\n",
    "        output = FCN(img.view(batch_size,-1))\n",
    "        correct_guesses = output.data.max(1)[1].eq(label.data).sum()\n",
    "        \n",
    "        val_avg_acc  += correct_guesses\n",
    "        \n",
    "        if idx%100==0 and idx!=0:\n",
    "            print (\"\\t V-Progress: [{}/{}]\\r\".format(idx+1,len(val_datagen)) )   \n",
    "    \n",
    "    val_avg_acc = val_avg_acc*1./(batch_size*len(val_datagen))\n",
    "    \n",
    "    print( \"Results: T-Loss {0:2.5f} | T-Acc {1:3.4f}% | V-Acc {2:3.4f}%\".format(train_avg_loss, train_avg_acc*100., val_avg_acc*100.))\n",
    "    T_Acc_Data.append(train_avg_acc)\n",
    "    V_Acc_Data.append(val_avg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das obige Skript beinhaltet alles, was nötig ist, um ein Neuronales Netzwerk zu trainieren: WIr haben unseren Datengenerator `datagen()` (mehr dazu gleich), unser **FCN**, eine Kostenfunktion `F.cross_entropy()`(siehe vorheriges Tutorial) sowie als Optimierungsalgorithmus einen Standard-Gradientenminimierer, `optim.SGD()`, wobei _SGD_ kurz für _Stochastic Gradient Descent_ ist.\n",
    "\n",
    "**Dataloader**\n",
    "Der `Dataloader` selbst ist eine Hülle für unser `Dataset` welches wir weiteroben geschrieben haben. Die Aufgabe hier ist es einfach, das Rausziehen der Daten zu vereinfachen und zu parallelisieren. Die Parameter, die man ihm übergibt, sind dabei die `batch_size`, d.h. für wie viele Bilder der Gradient berechnet werden soll, bis die Netzwerkgewichte upgedatet werden, `num_workers`, welches der Anzahl der CPU-Kernen entspricht sowie `shuffle` und `remove_last`, welches für Durchmischen der Daten und entfernen von Batches sind, die nicht `batch_size` entsprechen.\n",
    "\n",
    "**Optimierer**\n",
    "Als Optimierer haben wir hier einen _Stochastic Gradient Descent_-Algorithmus verwendet, welcher auch im vorherigen Tutorial zu finden ist. Alternativen, die zum Teil wesentlich besser funktionieren, finden sich unter <http://pytorch.org/docs/master/nn.html#loss-functions>.\n",
    "Eine genauere Betrachtung verschiedener Optimierer werden wir, wenn die Zeit da ist, noch durchgehen. Ansonsten einfach ausprobieren!\n",
    "\n",
    "Andere wichtige **Schlüsselwörter** sind im Code selbst erklärt!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Aufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geht durch die Beschreibungen auf <http://pytorch.org/docs/master/nn.html> und schaut, ob ihr Änderungen findet, die das Netz evtl. besser trainieren lassen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aufgabe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nehmt einen Trainingsdurchgang auf und plottet Kosten/Genauigkeit gegen Epoche oder Iteration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Acc_Data=[0.8347023809523809, 0.8354166666666667, 0.8460714285714286, 0.8496428571428571, 0.85, 0.9113095238095238, 0.9233333333333333, 0.9305952380952381, 0.93125, 0.9369642857142857]\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "x=[]\n",
    "y=[]\n",
    "x2=[]\n",
    "y2=[]\n",
    "for i,n in enumerate(T_Acc_Data):\n",
    "    x.append(i)\n",
    "    y.append(n)\n",
    "for i,n in enumerate(V_Acc_Data):\n",
    "    x2.append(i)\n",
    "    y2.append(n)\n",
    "plt.plot(x,y,\"r\")\n",
    "plt.plot(x2,y2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bis hierhin haben wir uns mit **FCN**s auseinandergesetzt, welche versuchen (zumindest in stark vereinfachenden Zügen) das Gehirn und seine Neuronen nachzustellen. Eine zweite Klasse, die solche Netzwerke aus einem rein mathematischen Standpunkt angeht, sind sog. **Convolutional Neural Network**s **(CNN)**s.\n",
    "\n",
    "Anders als bei unseren FCNs haben wir es hier mit **Filtern** statt Neuronen zu tun. Zudem nehmen sie als Eingangssignal direkt Bilder oder andere volumetrische Daten. Die Grobstruktur sieht aus wie in der unteren Abbildung (Quelle: >https://www.kdnuggets.com/2015/11/understanding-convolutional-neural-networks-nlp.html>).\n",
    "\n",
    "<img src=\"Bilder/cnn.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine ergänzende Darstellung (entnommen von <https://stackoverflow.com/questions/42733971/convolutional-layer-to-fully-connected-layer-in-cnn>) findet sich ebenso:\n",
    "\n",
    "<img src=\"Bilder/cnn2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beide Bilder (ansonsten einfach das Internet nach Bildern durchforsten) reichen hoffentlich aus, um die Wirkungsweise solche Netzwerke klarzustellen.\n",
    "\n",
    "Wir starten mit unserem Bild (in unserem Fall das Bild einer Zahl) und füttern es in ein **CNN**. Dieses hat, ähnlich wie das **FCN**, eine Reihe von Schichten:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist die Hauptschicht (ähnlich den Neuronen im FCN). Eine Hauptschicht ist eine Ansammlung sog. Filter, kleine Matrizen mit Gewichten, welche jeweils über das Bild geschoben werden und eine Convolution/Faltung berechnen (mathematisch eigentlich eine Korrelation, aber das tut nichts zur Sache). Das sieht etwa so aus:\n",
    "\n",
    "<img src=\"Bilder/conv_pic_example.png\">\n",
    "\n",
    "Wie man erkennen kann, nimmt ein Filter eine Pixelgruppe und gewichtet die Werte zu *einem* Aktivierungswert. Macht er das für alle Pixelgruppen auf einem Bild, entsteht eine sog. **Feature map** - eine Abbildung des Bildes zu einem Bild welches besondere Bildeigenschaften hervorhebt, z.B. Kanten.\n",
    "\n",
    "Da sich in jeder Schicht mehrere solcher Filter befinden, gibt es auch entsprechend viele solcher **Feature maps**. In obigen Bild ist dies durch die Breite (_96, 256, ..._) angegeben, da am Ende einer Schicht alle dieser feature maps verkettet werden, denn:\n",
    "\n",
    "In der nächsten Schicht nehmen sich die Filter wieder Pixelgruppen vor, dieses mal aber für alle Pixel in allen feature maps innerhalb einer Gruppenposition.\n",
    "\n",
    "**Ein Beispiel:**\n",
    "Seien in der ersten Schicht 20 Filter vorhanden. Das Eingangsbild hat die Maße $224\\times 224$. Die Filter haben je die Größe $3\\times3$. Nach Anwenden verbleiben wir mit 20 $222\\times222$-Bildern (die reduzierte Größe liegt daran, das an den Bildkanten der Filter etwas verschoben werden muss und so die Randpixel nicht gleich oft wie innere Pixel erfasst werden). Nach Verkettung macht das einen $20\\times222\\times222$-Klotz.\n",
    "In der nächsten Schicht haben dann die Filter die Ausmaße $20\\times3\\times3$, da die Informationen aus **allen** feature maps zusammengeführt werden!\n",
    "\n",
    "**Anmerkung**: Um die Bildverkleinerung zu bekämpfen, kann man stets das Eingangsbild/die Eingangsfeaturemaps an den Seiten mit Nullen ausstatten, und die Größe vor den Berechnungen etwas erhöhen, sodas am Ende wieder die gleiche Größe herauskommt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Anzahl an Gewichten zu reduzieren, führt man ganz gerne **Pooling Layer** ein. Das sind Minischichten, deren einzige Aufgabe es ist, die Ausmaße des Netzes zu reduzieren. Sie definieren sich über eine **Kerngröße/Kernelsize** ähnlich den Filtern oben. Sie nehmen sich eine Pixelgruppe vor und führen eine Operation aus, z.B. den Maximalwert oder das arithmetische Mittel nehmen. Entsprechend nennt man solche Schichten auch **Maxpooling**- bzw. **Averagepooling**-**Layer**.\n",
    "\n",
    "**Ein Beispiel**:\n",
    "Ein Satz Featuremaps der Größe $20\\times200\\times200$ hat nach einem *Maxpoolinglayer* mit *Kernelsize* $2\\times2$ die Größe $20\\times100\\times100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Sonstige Schichten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ansonsten sind die restlichen Schichten sehr ähnlich zu denen in einem **FCN**, d.h. eine Schicht mit einer Aktivierungsfunktion und/oder andere Funktionen.\n",
    "\n",
    "Zudem ist es nicht unüblich, das **CNN** mit einem kleinen **FCN** abzuschließen. Hierfür werden die letzten Featuremaps entsprechend vektorisiert (d.h. bei einem finalen Featureklotz der Größe $20\\times15\\times15$ wird ein Vektor der Länge $4500$ ausgespuckt) und in ein **FCN** gesteckt.\n",
    "\n",
    "Das ist auch in den obigen Bildern zu sehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wollen wir uns wieder an die Implementierung machen und schauen, wie sich diese Netzwerkstruktur im Vergleich zu einem **FCN** schlägt. Natürlich verwenden wir wieder PyTorch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Für das Netzwerk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitiv würde man meinen, dass man die Notation von oben verwenden, um das Netz zu implementieren, d.h etwas in diser Richtung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "CNN = torch.nn.Sequential(\n",
    "        nn.Conv2d(1,10, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)),\n",
    "        nn.Conv2d(10,20, kernel_size=5),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d((2,2)), #Hier fehlt der Übergang von Bild- zu Vektorstruktur    \n",
    "        nn.Linear(320,50),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(50,10),\n",
    "        nn.Softmax(dim=1)\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allerdings kann so nur umständlich der Übergang von Bild- zu Vektorstruktur erreicht werden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daher werden wir hier noch eine andere PyTorch-Notation einführen, mit der man kompliziertere Netze aufbauen kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN_Base(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Base,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=9)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(20,40, kernel_size=1)\n",
    "        self.fc1 = nn.Linear(640, 700)\n",
    "        self.fc2 = nn.Linear(700, 700)\n",
    "        self.fc3 = nn.Linear(700, 10)\n",
    "        self.out_act = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.leaky_relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = F.leaky_relu(self.conv3(x), 2)\n",
    "        x = x.view(-1, 640)   #Hier ändern wir Bild- zu Vektorformat\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.out_act(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ist alles aufgeteilt in zwei Teile:\n",
    "Innerhalb der `__init__()`-Funktion werden alle Netzschichten initialisiert, die Gewichte besitzen (d.h. mit zufälligen Gewichten ausgestattet, ähnlich einen Lexikon verfügbarer Bauklötze.\n",
    "\n",
    "Die tatsächliche Verknüpfung findet dann in `forward()` statt, d.h. hier wird dann mit den im Lexikon zu findenden Bauklötzen das eigentliche Netzwerk aufgebaut.\n",
    "\n",
    "Das erlaubt es, sehr komplizierte Strukturen zu implementieren. Die Funktionsweise und die Verwendung im allgemeinen Programm bleibt aber die Gleiche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Für das allgemeine Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir nur das Netzwerk abgeändert haben, können wir einfach die **FCN**-Trainingsstruktur kopieren und einfach unser neues Netzwerk einfügen. Die einzigen Änderungen die zu beachten sind ist die Tatsache, dass die Bilder nun nicht mehr Vektoren, sondern einfach Bilder sind. Dafür müssen wir aber lediglich die `img.view(batch_size,-1)`-Zeile entfernen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Epoch 0...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.05714 | T-Acc 64.2292% | V-Acc 91.4241%\n",
      "Training in Epoch 1...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.04741 | T-Acc 94.4583% | V-Acc 96.0162%\n",
      "Training in Epoch 2...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.04700 | T-Acc 95.6875% | V-Acc 96.1474%\n",
      "Training in Epoch 3...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.04701 | T-Acc 95.6607% | V-Acc 95.0024%\n",
      "Training in Epoch 4...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.04729 | T-Acc 94.7827% | V-Acc 93.2968%\n",
      "Training in Epoch 5...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.05333 | T-Acc 75.4643% | V-Acc 31.3454%\n",
      "Training in Epoch 6...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07291 | T-Acc 12.8095% | V-Acc 10.4723%\n",
      "Training in Epoch 7...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 8...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 9...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 10...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 11...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 12...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 13...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 14...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 15...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 16...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 17...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 18...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n",
      "Training in Epoch 19...\n",
      "\t T-Progress: [301/1050]\n",
      "\t T-Progress: [601/1050]\n",
      "\t T-Progress: [901/1050]\n",
      "\t V-Progress: [101/262]\n",
      "\t V-Progress: [201/262]\n",
      "Results: T-Loss 0.07368 | T-Acc 10.3363% | V-Acc 10.4723%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\"\"\" Hyperparameter \"\"\"\n",
    "batch_size = 32\n",
    "n_epochs   = 10\n",
    "learning_rate = 0.08\n",
    "train_validation_split = 0.8\n",
    "\n",
    "\"\"\" Aufsetzen der Datengeneratoren \"\"\"\n",
    "path_to_MNIST_dataset = \"TrainingSet\"\n",
    "\n",
    "all_image_paths, all_image_labels = get_image_paths(path_to_MNIST_dataset)\n",
    "\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_paths)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_image_labels)\n",
    "\n",
    "split_idx = int(len(all_image_paths)*train_validation_split)\n",
    "\n",
    "training_img_paths = all_image_paths[:split_idx]\n",
    "training_labels    = all_image_labels[:split_idx]\n",
    "train_dataset = MNIST_Data_Provider(training_img_paths, training_labels)\n",
    "train_datagen = DataLoader(train_dataset, batch_size=batch_size,drop_last=True, shuffle=True, num_workers=0)\n",
    "\n",
    "validation_img_paths = all_image_paths[split_idx:]\n",
    "validation_labels    = all_image_labels[split_idx:]\n",
    "val_dataset = MNIST_Data_Provider(validation_img_paths, validation_labels)\n",
    "val_datagen = DataLoader(val_dataset, batch_size=batch_size, drop_last=True, shuffle=False, num_workers=0)\n",
    "\n",
    "\"\"\" Laden des Netzes, Aufsetzen des Optimierers\"\"\"\n",
    "CNN = CNN_Base()\n",
    "#optimizer = optim.SGD(FCN.parameters(), lr=learning_rate)\n",
    "optimizer = optim.SGD(CNN.parameters(), lr=learning_rate)\n",
    "\n",
    "\"\"\" Training & Validierung \"\"\"\n",
    "\n",
    "t_Acc_V=[]\n",
    "v_Acc_V=[]\n",
    "for epoch in range(n_epochs):\n",
    "    print( \"Training in Epoch {}...\".format(epoch))\n",
    "    \n",
    "    \"\"\" Hier startet das Training! \"\"\"\n",
    "    #FCN.train()\n",
    "    CNN.train()\n",
    "    \n",
    "    train_avg_loss = 0\n",
    "    train_avg_acc  = 0\n",
    "    \n",
    "    for idx, (img,label) in enumerate(train_datagen):\n",
    "        img, label = Variable(img), Variable(label)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        #output = FCN(img.view(batch_size,-1))\n",
    "        output = CNN(img)\n",
    "        \n",
    "        loss = F.cross_entropy(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct_guesses = output.data.max(1)[1].eq(label.data).sum()\n",
    "        \n",
    "        train_avg_loss += loss.data[0]\n",
    "        train_avg_acc  += correct_guesses\n",
    "        \n",
    "        if idx%300==0 and idx!=0:\n",
    "            print( \"\\t T-Progress: [{}/{}]\\r\".format(idx+1,len(train_datagen)))\n",
    "\n",
    "    train_avg_loss = train_avg_loss*1./(batch_size*len(train_datagen))\n",
    "    train_avg_acc  = train_avg_acc*1./(batch_size*len(train_datagen))\n",
    "    \n",
    "    \"\"\" Hier startet die Validierung \"\"\"\n",
    "    #FCN.eval()\n",
    "    CNN.eval()\n",
    "    \n",
    "    val_avg_acc = 0\n",
    "    \n",
    "    for idx, (img,label) in enumerate(val_datagen):\n",
    "        img, label = Variable(img), Variable(label)\n",
    "        #output = FCN(img.view(batch_size,-1))\n",
    "        output = CNN(img)\n",
    "        correct_guesses = output.data.max(1)[1].eq(label.data).sum()\n",
    "        \n",
    "        val_avg_acc  += correct_guesses\n",
    "        \n",
    "        if idx%100==0 and idx!=0:\n",
    "            print (\"\\t V-Progress: [{}/{}]\\r\".format(idx+1,len(val_datagen)))   \n",
    "            \n",
    "    val_avg_acc = val_avg_acc*1./(batch_size*len(val_datagen))\n",
    "    \n",
    "    print(\"Results: T-Loss {0:2.5f} | T-Acc {1:3.4f}% | V-Acc {2:3.4f}%\".format(train_avg_loss,train_avg_acc*100.,val_avg_acc*100.))\n",
    "    t_Acc_V.append(train_avg_acc);v_Acc_V.append(val_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bdf9f65860>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bdf9f65438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=[]\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "#np.lookfor(\"create array numbers\")\n",
    "for i in range(len(t_Acc_V)):\n",
    "    x.append(i)\n",
    "plt.plot(x,t_Acc_V)\n",
    "plt.plot(x,v_Acc_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit haben wir nun ein weiteres Werkzeug eingeführt, um Bilderkennung zu implementieren.\n",
    "\n",
    "Generell ist es so, dass Netzwerk-Strukturen je nach Bedarf und Zweck entsprechend abgeändert werden. Generell gilt aber: Je tiefer das Netzwerk, desto ausdrucksstärker ist es, aber desto länger, komplizierter und anspruchsvoller ist es, dieses zu sinnvollen Werten trainiert zu bekommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Aufgaben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisier auch hier das Trainingverhalten. Vergleiche dies mit der **FCN**-Struktur von oben. Idealerweise werden beide Trainingskurven in ein Diagramm geplottet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ändere etwas an der Struktur oder dem Optimierungsalgorithmus ab um höhere Ergebnisse erzielt zu bekommen. Wie immer ist die PyTorch-Homepage dein Freund!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Aufgabe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vergleiche die Anzahl der Paramter zwischen **FCN** und **CNN**. Verwende dazu folgenden Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_parameters(net):\n",
    "    net_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "    num_params = sum([np.prod(p.size()) for p in net_parameters])\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Damit haben wie die Grundlagen zu neuronalen Netzen abgeschlossen!**\n",
    "\n",
    "Im nächsten und letzten Einführungsteil wollen wir uns noch mit Methoden auseinandersetzen, das Generalisierungsverhalten und das allgemeine Training zu verbessern."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
