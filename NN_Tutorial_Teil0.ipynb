{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neurale Netzwerke - Tutorial Teil 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Vorbereitungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit ihr optimal durch dieses Tutorial gehen könnt, ist es wichtig, die vorherigen Tutorials, nämlich zu Python und Linearer Algebra durchgegangen und in mindestens groben Zugen verstanden zu haben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Theorie [1/2]: Netzwerk Basics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Aufbau eines neuronalen Netzwerkes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein anwendungsbezogenes Verständnis aufzubauen, wollen wir uns zu Beginn mit der Grobstruktur eines einfachen neuronalen Netzwerkes beschäftigen, nämlich eines __fully-connected neural networks (FCN)__.\n",
    "Eine schematische Darstellung findet sich in <img src=\"Bilder/fnc.jpeg\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier sei schonmal angemerkt das die allgemeine Online-Community auf diesem Fachgebiet auf Englisch kommuniziert, daher werden wir uns auch hier viel der englischen Sprache bedienen.\n",
    "\n",
    "Auf diesem Bild zu sehen ist die grundlegende Struktur eines Netzes. Bedient man sich der Nervenzellen-Analogie, so stellt jeder weiße \"Blob\" ein Neuron da, welches mit anderen über eine spezielle mathematische Operation verbunden ist. Hier fängt die Analogie aber dann auch schon an zu schwächeln - in einem neuronalen Netzwerk sind diese Neuronen meist nach einem fixen Muster angeordnet und in ihren Verbindungen eingeschränkt; so kann ein mathematischen Neuron in der obigen Struktur nur mit räumlich vor- und nachgestellten Neuronen kommunizieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Neuronale Netzwerkschichten (_Layers_)\n",
    "Diese Anorderung hat oft eine Schichtform, daher auch die englische Bezeichnung __layer__. Je nach Position diser Layer übernehmen die Neuronen unterschiedliche Aufgaben. In groben Zügen unterscheidet man hier 3 Variationen:\n",
    "-  Eingangsschicht (_input layer_): Die Aufgaben der Neuronen hier ist es lediglich, die Eingangsdaten, auf welche wir das Netzwerk anwenden wollen, zu repräsentieren und nimmt schlicht die Eingangswerte an. Hiervon gib es in einem neuronalen Netzwerk, von hieran mit __NN__ abgekürzt, meist nur eines. Das wird klarer, wenn wir uns zum Beispiel vorgearbeitet haben. \n",
    "-  Versteckte Schichten (_hidden layers_): In diesen Layern findet die eigentliche Magie statt, in welcher die Eingangsinformation (_input_) abstrahiert wird und Strukturen erlernt werden. Meist sind hierbei mehrere _hidden layer_ nacheinander gesetzt. Diese heißen so, weil man mit ihnen nie \"direkt\" in Kontakt tritt; dass Signal schieben wir in den _input layer_ und wir messen nur was aus dem _output layer_ herauskommt.\n",
    "-  Ausgabeschicht (_output layer_): Dieser Layer rekonstruiert aus den gelernten Strukturen in den _hidden layern_ die gewünschte Ausgabe-Information (_output_). Hat man beispielsweise die Aufgabe, Bilder von Zellen in Tumorzellen und gesunde Zellen zu trennen, so kann der _output layer_ aus lediglich einem Neuron bestehen und nur eine Zahl (0 und 1 für die entsprechende Klasse) zurückgeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Die Verknüpfungen (_Connections_)\n",
    "Bis hierher haben wir viel über diese Neuronen geredet, können uns aber wenig darunter vorstellen. Tatsächlich lassen sich diese aber am Besten zusammen mit den Verbindungen erklären.\n",
    "\n",
    "Hierbei betrachten wir jedes Neuron als eine Art Mini-Taschenrechner. Bekommt dieses Neuron nun ein Signal eines anderen Neurons über die Verbindung zugeschickt, verarbeitet es dieses mathematischen Regeln entsprechend und schickt dann ein eigenes Signal weiter. Die Mathematik findet also in den Neuronen selbst ab; die Verbindungen in solchen Skizzen symbolisieren lediglich, zu welchen anderen Neuron das Signal weiter gegeben wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Aufbau eines Neurons\n",
    "Und so ist ein einzelnes Neuron aufgebaut (Bemerkung: das schließt im Allgemein die Eingangsneuron aus, welche lediglich die Eingangswerte annehmen):\n",
    "<img src=\"Bilder/single_neuron.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die neu eingeführten Variablen beschreiben dabei folgendes\n",
    "- Die angegebenen $x_i$ stehen dabei für die Eingangssignale, die von einem Neuron aus der vorhergehenden Schicht weitergegeben werden, wobei $n$ die Anzahl der insgesamt eingehenden Verknüpfungen ist.\n",
    "- $w_i$, $i\\in[1,...,n]$, symbolisiert dabei eine Reihe an Gewichtungen, welche für jedes Neuron einzigartig ist.\n",
    "- $b$ ist dabei der Neuron-spezifische Offset (_bias_).\n",
    "- $f$ ist eine Funktion (_activation function_), die auf das Ausgangssignal des Neurons angewandt wird, um es abschließend nochmal zu manipulieren und in die Ausgabe $y$ zu transformieren, welche dann an folgende Neuronen weitergegeben wird (als neues $x_i$).\n",
    "\n",
    "Für gegebene Eingangswerte [$x_1$,$x_2$,...,$x_n$], den Gewichtungen [$w_1$,$w_2$,...,$w_n$], dem _bias_ $b$ sowie der Aktivierungsfunktion $f$ wird folgende Operation im Neuron durchgeführt:\n",
    "\n",
    "\\begin{equation}\n",
    "y = f\\left( b + \\sum_{i=1}^n w_ix_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Die Interpretation hierfür ist folgende: Das Neuron schaut sich alle Signale seiner Vorgänger an und beschließt über die Gewichtungen selbst, für wie wichtig er diese jeweils hält; der _bias_ $b$ entspricht dabei einer grundlegenden Einstellung des Neurons, allgemein höhere oder niedrigere Werte auszugeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Rolle der Aktivierungsfunktion\n",
    "Dabei sind wir noch nicht auf die Aktivierungsfunktion eingegangen. Diese nimmt die gewichtetet Summe der Eingangssignale und projeziert sie auf einen anderen Wert. Der Grund hierfür ist der folgende:\n",
    "\n",
    "Summen, welche von allen Neuronen durchgeführt werden, sind linearer Natur; genauso wie es aber auch Summen von Summen von Summen (lineare Bausteine $b$ und $w_ix_i$) sind (was im Netzwerk exakt passieren würde, wenn man die Aktivierungsfunktion weglässt).\n",
    "Das bedeutet, dass das Netzwerk nur lineare Sachverhalte modellieren kann; und das ist nicht ausreichend - schließlich soll es ja komplizierte, nicht lineare Formen wie Tumore erkennen können!\n",
    "\n",
    "Deswegen wendet man die __nichtlineare Aktivierungsfunktion__ an - denn dann sind die Ergebnisse nichtlinearer Natur!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Aufgabe\n",
    "Zeige das ohne Aktivierungsfunktion das Ergebnis verknüpfter Neuronen wieder linear ist! Zeige außerdem dass dies nicht mehr der Fall ist, wenn man eine nichtlineare Aktivierungsfunktion anhängt. Mache das einfach anhand eines Beispiels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Funktionen $f(x) = a\\cdot x+b$ und $g(x) = c\\cdot x+d$ findet sich:  \n",
    "\n",
    "$f(g(x)) = a\\cdot(c\\cdot x + d) + b = ac\\cdot x + ad + b$\n",
    "\n",
    "was wiederum linearer Natur ist!\n",
    "\n",
    "Sei nun $h(x) = \\exp{x}$. Dann ist\n",
    "\n",
    "$f(h(g(x))) = a\\cdot(\\exp{(c\\cdot x+d)}) + b = a\\cdot \\exp{(c\\cdot x+d)} + b$\n",
    "\n",
    "nicht mehr linear!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Typen von Aktivierungsfunktionen\n",
    "Beliebte Aktivierungsfunktionen, um ein nichtlineares Verhalten zu erzwingen, sind folgende Beispiele (sei hierbei $z =  b + \\sum_{i=1}^n w_ix_i$):\n",
    "1. __ReLU__ (_Rectified linear unit_): \n",
    "\\begin{equation}\n",
    "y = f(z) = max(0,z) \\in [0,z]\n",
    "\\end{equation}\n",
    "<img src=\"Bilder/relu.png\">\n",
    "2. __Sigmoid__ function: \n",
    "\\begin{equation}\n",
    "y = \\frac{1}{1+e^{-z}} \\in [0,1]\n",
    "\\end{equation}\n",
    "<img src=\"Bilder/sigmoid.png\">\n",
    "3. __Tanh__ (_tangens hyberbolicus_): \n",
    "\\begin{equation}\n",
    "y = \\frac{e^x-e^{-x}}{e^x+e^{-x}} \\in [-1,1]\n",
    "\\end{equation}\n",
    "<img src=\"Bilder/tanh.png\">\n",
    "\n",
    "In Studien zeigt sich allerdings, dass _ReLU_ als Aktivierungsfunktion für versteckte _layer_ in den meisten Fällen am Besten zu funktionieren scheint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Alle Neuronen zusammenführen\n",
    "Nun wissen wir, wie ein einzelnes Neuron arbeitet. Die Frage ist nun, wie man das am Besten mit einem ganzen Netzwerk solcher Neuron anstellt.\n",
    "\n",
    "Die Antwort hierfür bietet uns überraschenderweise die lineare Algebra, welche wir glücklicherweise im vorherigen Tutorial durchgegangen sind.\n",
    "Die Frage ist nämlich: Kann ich die Summenoperation, welche im Neuron durchgeführt wird, auch umschreiben?\n",
    "Klarerweise lautet die Antwort ja, und zwar als Skalarprodukt der Form:\n",
    "\n",
    "\\begin{equation}\n",
    " b + \\sum_{i=1}^n w_ix_i  =  b + \\textbf{w}^T\\cdot \\textbf{x} \n",
    "\\end{equation}\n",
    "\n",
    "mit Gewichtsvektor $\\textbf{w}=\\begin{pmatrix} w_1\\\\w_2\\\\...\\\\w_n \\end{pmatrix}$ und _input_-vektor $\\textbf{x} = \\begin{pmatrix} x_1\\\\x_2\\\\...\\\\x_n \\end{pmatrix}$.\n",
    "\n",
    "Nun müssen wir eine wichtige Annahme machen, nämlich das alle Neuronen eine vorherigen Schicht mit allen Neuronen der folgenden Schicht verbunden sind. Ist dies nämlich der Fall, so ist __x__ gleich für alle Neuronen einer Schicht! \n",
    "Definieren wir uns nun $\\textbf{y}^j = \\begin{pmatrix} y_1\\\\y_2\\\\...\\\\y_m \\end{pmatrix}$ als Sammelvektor aller Ausgangswerte der Neuronen in der _j_-ten Schicht, so ist dieser gegeben über:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\textbf{y}^j = f\\left( \\textbf{b}^j + \\textbf{W}^j\\cdot \\textbf{x}^{j-1}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "D.h. wir sammeln alle Ausgangswerte der Neuronen der vorausgehenden Schicht in $\\textbf{x}$ und \n",
    "multiplizieren ihn mit der Gewichtungsmatrix $\\textbf{W}$. Die Aktivierungsfunktion $f$ wird dabei auf jedes Element im resultierenden Vektor angewandt.\n",
    "Man bemerke, dass $\\textbf{W}$ nichts weiter als eine Verkettung aller Gewichtsvektoren der Neuronen in der _j_-ten Schicht ist, also:\n",
    "\\begin{equation}\n",
    "\\textbf{W}^j = [\\textbf{w}^{j^T}_1, \\textbf{w}^{j^T}_2, ..., \\textbf{w}^{j^T}_n]\n",
    "\\end{equation}\n",
    "\n",
    "Hier ist also $\\textbf{w}^{j^T}_1$ definiert als der Vektor aller Gewichte des $\\textit{i}$-ten Neurons in der $\\textit{j}$-ten Schicht mit einer Transponierung, damit das Skalarprodukt mit __x__ definiert ist. \n",
    "Entsprechend setzt sich auch der _bias_ zusammen als\n",
    "\\begin{equation}\n",
    "\\textbf{b}^j = [b^j_i, b^j_2, ..., b^j_n]\n",
    "\\end{equation}\n",
    "sprich ein einzelner Vektor mit skalaren Offsetwerten für jedes Neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Ein Beispiel:__ Wählen wir 784 Eingangneuronen, 30 versteckte und 10 Ausgangsneuronen, so haben wir zwei Gewichtsmatrizen und zwei Offsetvektoren: Die erste Matrix, $\\textbf{W}^1$, welche die Eingangsdaten auf die versteckten Neuronen abbildet, und die Zweite, welche aus den versteckten Neuronen unsere Ausgangsschicht kreiert. Die Matrizen haben damit die Größen $30\\times784$ sowie $10\\times30$. Die Offsetvektoren haben entsprechend die Größen $30\\times1$ und $10\\times1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Aufgabe\n",
    "Um das ganze etwas zu vertiefen, hier eine kleine Aufgabe: \n",
    "Berechnet für ein Mininetz mit 3 Schichten (einer Inputschicht und zwei _hidden layern_) welche folgende Werte haben:\n",
    "-  Inputwerte: $\\textbf{x}$= [1,2]\n",
    "-  _Hidden Layer_ 1: $\\textbf{W}^1_1 = [0.5,1.5]$ sowie $\\textbf{W}^1_2 = [1.5,0.5]$ und $\\textbf{b}^1 = [1,0]$\n",
    "-  _Hidden Layer_ 1: $\\textbf{W}^2_1 = [0,7]$ sowie $\\textbf{W}^2_2 = [-1,1]$ und $\\textbf{b}^2 = [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Lösung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Ergebnisse der beiden Neuronen im 1. Layer__:  \n",
    "\n",
    "    Neuron 1: $\\textbf{W}^1_1 \\cdot \\textbf{x} + \\textbf{b}^1_0 = 4.5$\n",
    "    \n",
    "    Neuron 2: $\\textbf{W}^1_2 \\cdot \\textbf{x} + \\textbf{b}^1_1 = 3.5$ --nicht 2,5?\n",
    "    \n",
    "    \n",
    "2. __Ergebnisse der beiden Neuronen im 2. Layer__:\n",
    "    \n",
    "    Die Inputwerte kommen nun aus dem vorhergenden Layer: $\\textbf{x}=[4.5,3.5].$\n",
    "    \n",
    "    Neuron 1: $\\textbf{W}^2_1\\cdot \\textbf{x} + \\textbf{b}^2_0 = 24.5$\n",
    "    \n",
    "    Neuron 2: $\\textbf{W}^2_2\\cdot \\textbf{x} + \\textbf{b}^2_1 = 0$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Outputlayer\n",
    "Der letzte Layer hat meist noch eine Besonderheit im Vergleich zu seinen versteckten Kollegen, nämlich das es oft der Fall ist, dass unsere Ausgabe bestimmte Werte annehmen soll, zum Beispiel zwischen 0 und 10.\n",
    "\n",
    "Um das besser zu verstehen, ziehen wir Teile des Beispiels vor, welches wir im Späteren noch durcharbeiten werden: Das Erkennen von Ziffern zwischen 0 und 9. Schicken wir nun unser Bild in das Netzwerk, so muss es ja Werte herausgeben, aus denen wir lesen können, für welche Zahl es sich entschieden hat. \n",
    "\n",
    "Dafür geht man meist so vor, dass man die Anzahl der Neuron im _output layer_ der Anzahl an vorherzusagenden Klassen, also 10 in diesem Fall, anpasst. Wenn wir das machen, so gibt uns das Netzwerk 10 Werte zurück, wo die Position der Werte im Ausgabevektor der entsprechenden Zahl gehörig ist, uns die Werte meist der Sicherheit des Netzes entspricht, dass der Input zur entsprechenden Klasse gehört.\n",
    "\n",
    "Konkret an einem Beispiel:\n",
    "Gibt uns das Netzwerk den Vektor $out = \\begin{pmatrix} 1\\\\1.2\\\\2\\\\0.3\\\\1\\\\2\\\\10\\\\3.4\\\\5.1\\\\-2 \\end{pmatrix}$, so geht es davon aus, dass der Input mit hoher Wahrscheinlichkeit der Zahl 6 entspricht.\n",
    "\n",
    "Apropos Wahrscheinlichkeit: Die Ausgabe in dieser Form ist noch keine Wahrscheinlichkeit, da sie nicht auf 1 normiert ist. Um das zu ermöglichen, wählt man als finale Aktivierungsfunktion gerne sie sogenannte _Softmax_-Funktion, welches für jedes Element\n",
    "\\begin{equation}\n",
    "    f(z_i) = \\frac{e^{z_i}}{\\sum_{k=0}^K e^{z_k}}\n",
    "\\end{equation}\n",
    "berechnet.\n",
    "Sprich, es rechnet den Exponenten für alle Ausgabeelemente aus und normiert diese dann anschließend. \n",
    "\n",
    "\n",
    "__Damit haben wir uns bereits durch den kompletten Aufbau eines einfachen Netzwerkes durchgearbeit!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Theorie [2/2]: Trainieren des Netzwerkes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun stellen sich folgende zwei Fragen, die wir in diesem Teil adressieren wollen: Wie trainiere ich das Netzwerk, damit es lernt, dass zu tun was ich möchte? Und mit welchen Werten für die Gewichte und den _bias_ soll ich starten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fangen wir mit dem Beantworten der ersten Frage an - wie trainiere ich das Netzwerk? Aber bevor wir überhaupt da einsteigen können müssen wir uns vergewissern, dass unser Datensatz eine verwendbare Struktur besitzt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Datenstruktur\n",
    "Wir brauchen primär natürlich representative Daten für das Problem, welches wir mittels unseres NNs lösen wollen. \n",
    "\n",
    "Im Falle der Ziffernklassifikation wären das zum Beispiel Schwarzweißbilder von handgeschrieben Zahlen, zum Beispiel in der Form $28\\times 28$. Nun können wir das aber so nicht wirklich praktisch in unser geschichtetes Netzwerk einlesen, daher transformieren wir den Datensatz so, dass die Bilder schlicht durch einen einzigen Vektor der Länge $784$ dargestellt wird, in dem wir alle Pixel einfach aneinanderreihen.\n",
    "\n",
    "Das reicht aber zum Trainieren noch nicht; wir müssen dem Netzwerk noch mitteilen, was für eine Nummer er grade gefüttert bekommt. Aber ihm einfach eine Zahl zu geben reicht nicht; wir haben uns schließlich die vektorielle Ausgabe ausgesucht!\n",
    "\n",
    "Daher wird die Zahl einfach als Vektor der Form $[0,0,0,1,...,0]^T$ geschrieben, mit einer Eins an der Position, welche dem Zahlenwert entspricht - so kann man die Ausgabe des Netzen nämlich direkt mit dem wahren Wert abgleichen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Kostenfunktion\n",
    "Und genau dieses \"Abgleichen\" müssen wir quantifizieren; indem wir ihm ein Mass dafür geben, wie weit seine Vermutung von der Wahrheit entfernt ist.\n",
    "\n",
    "Für so etwas definiert man sich eine sog. __Kostenfunktion (Lossfunction)__ $\\textbf{L}(\\textbf{o},\\textbf{t})$, welche die Ausgabe des Netzes $\\textbf{o}$ mit dem wahren Wert $\\textbf{t}$ abgleicht.\n",
    "\n",
    "Gerne dafür verwendete Funktion sind:\n",
    "1. ___MSE___ (_mean-squared error_): \n",
    "\\begin{equation}\n",
    "\\textbf{L} = \\frac{1}{len(t)}\\sum_{i=0}^{len(t)}(\\textbf{t}_i-\\textbf{o}_i)^2\n",
    "\\end{equation}\n",
    "2. ___CE___ (categorical cross-entropy): \n",
    "\\begin{equation}\n",
    "\\textbf{L} = \\frac{1}{len(t)}\\sum_{i=0}^{len(t)}(\\textbf{t}_i\\ln{\\textbf{o}_i}-(1-\\textbf{t}_i)\\ln{(1-\\textbf{o}_i)})^2\n",
    "\\end{equation}\n",
    "\n",
    "Hierbei benennen $\\textbf{o}_i$ und $\\textbf{t}_i$ schlichtweg die Elemente der Vektoren an der Position $i$.\n",
    "\n",
    "Erstere Funktion misst lediglich die quadratische Abweichung der Vorhersagen zum wahren Wert, während letztere den Informationsgehalt bemisst und aus der Informationstheorie stammt - genauere Details kann man gerne selbst nachlesen. Beide Funktionen (neben vielen anderen Funktionen) haben für ihre Probleme die entsprechende Daseinsberechtigung, für Klassifikationsprobleme hat sich aber oft die ___CE___ als Gewinner herausgezeichnet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Backpropagation and Stochastik Gradient Descent\n",
    "\n",
    "__Bemerkung: Es folgt der wohl mit Abstand anspruchsvollste Teil. Es ist super, diesen Teil zu verstehen, aber falls nicht, tut es der späteren Anwendung nicht viel ab.__\n",
    "\n",
    "Nun haben wir also ein Mass dafür, wie schlecht bzw. gut unser Netzwerk zu einem bestimmten Zeitpunkt ist. Es fehlt allerdings die bisher die Möglichkeit, diese Diskrepanz den Gewichten der Neuron mitzuteilen, denn diese sind es, die am Ende des Netz befähigen, Dinge zu erkennen!\n",
    "\n",
    "Und tatsächlich war das Lösen dieses Problems lange der Grund dafür, warum NNs im Schatten anderer Bilderkennungsalgorithmen blieben, bis Leute um _Geoff Hinton_, _Yoshua Bengio_ sowie _Yann LeCun_ eine Möglichkeit gefunden haben, dieses Problem zu lösen - nämlich mittels des heutzutage überall verwendeten __Backpropagation__-Algorithmuses!\n",
    "\n",
    "Die Aufgabe dieser Methode ist es, für jedes $w_{ik}^j$, also das _k_-te-Gewicht des _i_-ten Neurons in der _j_-ten Schicht, einen Wert zu finden, der verdeutlicht, wie dieses Gewicht abgeändert werden muss, damit ein besseres Ergebnis dabei herausspringt.\n",
    "\n",
    "Das Updaten sieht dabei so aus (für den _t_-ten iterationsschritt):\n",
    "-  Für die Gewichte: \n",
    "\\begin{equation}\n",
    "w_{ik}^{j(t+1)} = w_{ik}^{jt} - \\eta \\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial w_{ik}^{jt}}\n",
    "\\end{equation}\n",
    "-  Für die Offsets:\n",
    "\\begin{equation}\n",
    "b_{i}^{j(t+1)} = b_{i}^{jt} - \\eta \\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial b_{i}^{jt}}\n",
    "\\end{equation}\n",
    "\n",
    "Die hierfür wichtigen Elemente sind die Folgenden:\n",
    "- die __Lernrate__ $\\eta$: Sie gibt uns an, wie sehr wir in jedem Lernschritt unsere Gewichte verändern wollen; hohe Werte bedeuten, dass das Netz auf die kleinsten Ungereimtheiten in der Kostenfunktion reagiert, wohingegen kleine Werte langsamere Anpassung bedeutet.\n",
    "- die partielle Ableitung (_gradient_) der Kostenfunktion nach dem entsprechenden Gewicht \n",
    "$\\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial b_{i}^{jt}}$: Diese misst, wie sehr sich die Kostenfunktion bzw. eben die Vorhersage des Netzwerkes bei einer Änderung des entsprechenden Gewichtes ändert; ist diese Änderung groß, bedeutet dies, dass das Gewicht maßgeblich an der Vorhersage beteiligt war. War diese fehlerhaft, so war vermutlich dieses Gewicht stark daran beteiligt. Daher müssen wir dieses umso stärker abändern.\n",
    "Diese Form der Anpassung in Richtung besserer Werte nennt sich auch __Gradient Descent__.\n",
    "\n",
    "\n",
    "Diese partiellen Ableitungen sind für Neuronen in der gleichen Schicht (bei FCNs) von der gleichen Struktur - aber wie berechne ich diese für Neuronen die zum Beispiel ganz am Anfang des Netzes sich befinden, dass Netzwerk beliebig tief ist und die Kostenfunktion ja nur mit den Ergebnissen der letzten Schicht berechnet wird?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Kern des Ganzen findet sich im bereits erwähnten __Backpropagation__-Algorithmuses. Dieser berechnet den Wert des entsprechenden Gradient einfach, indem er die Kettenregel der Differentialrechung einfack sukzessiv anwendet.\n",
    "\n",
    "Zur Erinnerung: Für eine Funktion $f$, welche als Variable eine weitere Funktion $x(t)$ nimmt, berechnet sich die Ableitung nach $t$ als:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f(x(t))}{\\partial t} = \\frac{\\partial x(t)}{\\partial t}\\frac{\\partial f(x)}{\\partial x}\n",
    "\\end{equation}\n",
    "\n",
    "Setze sich unser Netzwerk nun aus $J$ verschieden Schichten zusammen, wobei J gleichzeitig die _output_-Schicht markiert. Dann berechnet sich der Gradient des _k_-ten Gewichtes im _i_-ten Neuron in der _j_-ten Schicht über eine wiederholte Anwendung der Kettenregel. __Allerdings__: Denken wir an das erste Bild zurück, so ist ein Neuron an der Ausgabe von ALLEN darauffolgenden Neuronen beteiligt. In unserer Kostenfunktion bedeutet das, dass sich in jedem Wert des Ausgabevektors $\\textbf{o}$ eine Abhängigkeit zum entsprechenden Gewicht zeigt. Deswegen muss man dieses Problem vektoriell angehen.\n",
    "\n",
    "Dafür definieren wir uns den Fehler (_error_) $\\delta^j$, welcher für jede Schicht ein Äquivalent der Kostenfunktion wiederspiegelt. Wohlgemerkt ist $\\delta^j$ ein Vektor, dessen Elemente $\\delta^j_i$ den Anteil am Gesamtfehler des _i_-ten Neurons der _j_-ten Schicht entspricht. \n",
    "\n",
    "Für die letzte Schicht ergibt sich dieser Fehlerbeitrag als\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^J = \\nabla_{\\textbf{o}} L(\\textbf{o},\\textbf{t}) \\odot \\frac{\\partial f}{\\partial z}(\\textbf{z}^J)\n",
    "\\end{equation}\n",
    "\n",
    "mit der Ableitung der Ausgangsaktivierungsfunktion $\\frac{\\partial f(z)}{\\partial z}$ und dem Gradienten der Kostenfunktion $\\nabla_{\\textbf{o}} L(\\textbf{o},\\textbf{t})$, der Neuronensumme $\\textbf{z}_j = b+\\sum_{k=1}^n w_{ik}^{j}x_{ik}^{j-1}$ sowie der elementweisen (NICHT Matrixmultiplikation) Multiplikation $\\odot$. Hier sei angemerkt, dass der Gradient nichts weiter als ein Vektor ist, welcher die Ableitungen der Kostenfunktion nach allen Elementen des Ausgabevektors __o__ enthält, also:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_{\\textbf{o}} L(\\textbf{o},\\textbf{t}) = \\begin{pmatrix} \\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial \\textbf{o}_1}\\\\ \\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial \\textbf{o}_2}\\\\ ... \\\\ \\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial \\textbf{o}_K} \\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "mit der Anzahl der Ausgabeneuronen K.\n",
    "\n",
    "Der Fehlerterm für alle vorhergehenden Schichten, $\\delta^j$, ergibt sich aus:\n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^j = ((\\textbf{W}^{j+1})^T \\cdot \\delta^{j+1}) \\odot \\frac{\\partial f}{\\partial z}(\\textbf{z}^j)\n",
    "\\end{equation}\n",
    "\n",
    "Der Schlüssel zum Verständnis ist hierbei die transponierte Gewichtsmatrix $(\\textbf{W}^{j+1})^T$. Wer sich noch erinnert, der weiß, dass die normale Gewichtsmatrix einfach die Eingangsgewichte jedes einzelnen Neurons in den Zeilen abgespeichert hat. Die Gewichte jedes Neurons bewerten dabei jeweils die Ausgaben aus den vorherigen Neuronen, die in das entsprechende Neuron eingelesen werden. Transponiere ich das und wende es auf den Fehler an, verteile ich damit proportional zu den Gewichten den Fehler $\\delta^j$ wieder an die entsprechenden Neuronen zurück.\n",
    "\n",
    "Und damit ist der __Backpropagation-Algorithmus fast vollständig__: Die entsprechenden Updategradienten der einzelnen Offsets und Gewichte ergeben sich aus\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial w_{ik}^{j}} = f^{j-1}(\\textbf{z}_{k}^{j-1})\\delta_i^j\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L(\\textbf{o},\\textbf{t})}{\\partial b_{i}^{j}} = \\delta_i^j\n",
    "\\end{equation}\n",
    "\n",
    "wobei $f^{j-1}(\\textbf{z}_{k}^{j-1})$ das _k_-te Element der aktivierten Ausgabe der vorherigen Schicht ist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wohlgemerkt haben wir all das bisher so konstruiert, dass wir die Gewichte nach jedem Trainingsbeispiel updaten. Gerne wird das aber auch so gehandhabt, dass man sich zufällig eine Anzahl Elemente aus dem Trainingsset aussucht und für alle __Backpropagation__ ausführt und dabei die Gradienten speichert. nachdem für alle Elemente die Gradienten berechnet wurden, wird das Ergebnis gemittelt und damit dann das Updaten durchgeführt. Benennen wir diese Minisets als __Minibatches__ mit Anzahl Elementen _bs_, so ist unsere neue Vorschrift:\n",
    "\n",
    "\n",
    "-  Für die Gewichte: \n",
    "\\begin{equation}\n",
    "w_{ik}^{j(t+1)} = w_{ik}^{jt} - \\frac{\\eta}{bs} \\sum_{s=1}^{bs}\\frac{\\partial L(\\textbf{o}_s,\\textbf{t}_s)}{\\partial w_{ik}^{jt}}\n",
    "\\end{equation}\n",
    "-  Für die Offsets:\n",
    "\\begin{equation}\n",
    "b_{i}^{j(t+1)} = b_{i}^{jt} - \\frac{\\eta}{bs} \\sum_{s=1}^{bs}\\frac{\\partial L(\\textbf{o}_s,\\textbf{t}_s)}{\\partial b_{i}^{jt}}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Diese Vorschrift nennt sich auch ___Stochastic Gradient Descent with minibatches (SGDm)___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Werte initialisieren\n",
    "\n",
    "Wenn man für den ersten Iterationsschritt alle Werte mit 0 initialisiert, kann das Netzwerk nicht lernen, da die Updates dann auch stets 0 sind. Um diese Symmetrie zu brechen, kann man zum Beispiel mit einer Normalverteilung initialisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Aufgaben\n",
    "__Das hier ist die wohl anspruchsvollste Aufgabe in diesem Labjahr und erforder Verständnis von allem, was wir bisher hatten. Wer das löst, kann von sich behaupten, alles verstanden zu haben. Wohlgemerkt ist das eine längere Aufgabe! Wer Lösungen abgleichen möchte, schreibt mir einfach eine Mail!__\n",
    "\n",
    "\n",
    "Nehmen wir wieder unsere Mininetz aus der vorherigen Aufgabe mit\n",
    "-  Inputwerten: [1,2]\n",
    "-  _Hidden Layer_ 1: $\\textbf{w}^1_1 = [2,0]$ sowie $\\textbf{w}^1_2 = [2,1]$ und $\\textbf{b}^1 = [1,0]$\n",
    "-  _Hidden Layer_ 2: $\\textbf{w}^2_1 = [0,7]$ sowie $\\textbf{w}^2_2 = [-1,1]$ und $\\textbf{b}^2 = [0,1]$\n",
    "-  _Output Layer_ : $\\textbf{w}^3_1 = [3,2]$ sowie $\\textbf{w}^2_2 = [-1,-1]$ und $\\textbf{b}^3 = [1,1]$\n",
    "\n",
    "Als Aktivierung nehmen wir für ALLE Schichten die Sigmoid-Aktivierung $f(z) = \\frac{1}{1+e^{-z}} \\in [0,1]$ sowie als Kostenfunktion die _MSE_-Funktion.\n",
    "\n",
    "Skizziere das Netz und berechne die neuen Gewichte!\n",
    "\n",
    "\n",
    "___Alternativ___: Berechne die Ableitung der Sigmoidfunktion nach $z$, denn diese werden wir später benötigen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Abschließende Worte__: Damit schließen wir die Theorie zu komplett vernetzten neuronalen Netzwerken ab und widmen uns der Praxis. Diese erfordert glücklicherweise nicht das gesamte Verständnis, insbesondere wenn wir uns im nächsten Tutorial mit PyTorch beschäftigen. Es wird aber noch einen freiwilligen Theorieteil geben, wenn wir uns mit Faltungsnetzen (_Convolutional Neural Networks_) beschäftigen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Praxis: Ziffern klassifizieren\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun geht es darum, dass gelernte in die Tat umzusetzen. Dafür wollen wir ein einfaches Netzwerk in `numpy` schreiben, welches handgeschriebene Ziffern klassifiziert. Hier aber schonmal vorweg: `numpy` ist für so etwas denkbar ungeeignet, da die Gradienten alle von Hand berechnet werden müssen. Speziell dafür angepasste Bibliotheken wie `PyTorch`, womit wir uns danach beschäftigen, machen das automatisch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Daten einlesen\n",
    "Um überhaupt arbeiten zu können, müssen wir erst einmal in der Lage sein, die Daten einlesen zu können. Dafür ladet ihr aus dem Google Drive den MNIST Trainingsdatensatz herunter und entpackt ihn; idealerweise in dem Ordner, in dem sich diese Datei befindet.\n",
    "\n",
    "Nun starten wir das eigentliche Programmieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "path_mnist      = os.getcwd()+\"/trainingSet\"\n",
    "all_image_paths = []\n",
    "all_labels      = []\n",
    "for numberpath in os.listdir(path_mnist):\n",
    "    ### Unter MacOS-System werden in Ordnern Attributsdateien <.DS_Store> generiert, welche wir ignorieren wollen.\n",
    "    ### Für alle anderen Betriebssystem kann das ignoriert werden.\n",
    "    if numberpath != \".DS_Store\":\n",
    "        all_image_paths.extend([path_mnist+\"/\"+numberpath+\"/\"+x for x in os.listdir(path_mnist+\"/\"+numberpath)])\n",
    "        all_labels.extend([int(numberpath) for _ in range(len(os.listdir(path_mnist+\"/\"+numberpath)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamtzahl an Dateien: 42000\n"
     ]
    }
   ],
   "source": [
    "### Pfade gut durchmischen\n",
    "np.random.seed(1) #Reproduzierbarkeit beim Mischen garantieren\n",
    "np.random.shuffle(all_image_paths)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(all_labels)\n",
    "\n",
    "num_imgs = len(all_image_paths)\n",
    "print(\"Gesamtzahl an Dateien: {}\".format(num_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prozentsatz der Daten die wir verwenden wollen.\n",
    "percentage_data = 0.1\n",
    "\n",
    "train_imgs    = all_image_paths[:int(num_imgs*percentage_data)]\n",
    "train_targets = all_labels[:int(num_imgs*percentage_data)]\n",
    "val_imgs      = all_image_paths[int(num_imgs*percentage_data):int(num_imgs*(percentage_data+0.05))]\n",
    "val_targets   = all_labels[int(num_imgs*percentage_data):int(num_imgs*(percentage_data+0.05))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier teilen wir die Daten in einen Training uns einen Validierungssatz. Auf dem ersten trainieren wir das Netzwerk, während das Zweite die Aufgabe hat, zu schauen wie das Netzwerk auf ungesehenen Daten agiert. Um schneller Ergebnisse zu sehen, arbeiten wir außerdem nur auf 15% des gesamten Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hier schreiben wir uns einen sogenannten Generator. Dieser nimmt die Pfade und öffnet die Bilder nacheinander.\n",
    "### Das Reshape-Argument quetscht das geladene Bild zu einem langen Vektor zusammen, damit es vom NN \n",
    "### verwendet werden kann\n",
    "def datagen(img_paths, label_paths, bs=1, reshape=False):\n",
    "    seed = np.random.randint(100000)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(img_paths)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(label_paths)  \n",
    "    \n",
    "    img_sub_paths = [img_paths[i:i+bs] for i in range(0,len(img_paths),bs)]\n",
    "    batch_targets = [label_paths[i:i+bs] for i in range(0,len(label_paths),bs)]\n",
    "    \n",
    "    for im_sub_path, sub_labs in zip(img_sub_paths, batch_targets):\n",
    "        if not reshape:\n",
    "            yield [np.array(Image.open(im_path)) for im_path in im_sub_path], [lab for lab in sub_labs]\n",
    "        else:\n",
    "            yield np.vstack([np.array(Image.open(im_path)).reshape(1,-1) for im_path in im_sub_path]), np.array([lab for lab in sub_labs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generator erstellen für die Trainingsdaten zum Anschauen:\n",
    "train_gen = datagen(train_imgs, train_targets, bs=10, reshape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zahlen im Minibatch: [1 0 0 7 9 5 1 3 0 9]\n"
     ]
    }
   ],
   "source": [
    "### Visualisieren eines Beispiel-batches:\n",
    "ex_img, target = next(iter(train_gen))\n",
    "\n",
    "plt.imshow(ex_img[0].reshape(28,28), cmap=\"gray_r\")\n",
    "plt.title(\"Zahl: {}\".format(target[0]))\n",
    "plt.show()\n",
    "print(\"Zahlen im Minibatch: {}\".format(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da das Laden der Daten nun scheinbar funktioniert, wollen wir uns endlich dem Aufbau des Netzes widmen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Netzwerk in Numpy aufbauen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierfür, und vor allem für das Arbeiten mit `Pytorch` im Späteren ist es unablässlich, Klassen einzuführen. Für mich sind sie einer der schönsten Aspekte am Programmieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meine Eigenschaften: Auto und Brot\n",
      "Meine Eigenschaften: Nicht Auto und Kein Brot\n"
     ]
    }
   ],
   "source": [
    "### Eine Klasse in Python hat folgende Grundstruktur:\n",
    "\n",
    "class Klassenname():\n",
    "    def __init__(self, start_parameter1, start_parameter2):\n",
    "        self.eigenschaft1 = start_parameter1\n",
    "        self.eigenschaft2 = start_parameter2\n",
    "    \n",
    "    def funktion_auf_Klasse(self):\n",
    "        print(\"Meine Eigenschaften: {} und {}\".format(self.eigenschaft1, self.eigenschaft2))\n",
    "        \n",
    "### Nun generieren wir uns eine sogenannte Instanz der Klasse. Wenn man sich die Klasse als Bauplan vorstellt, so\n",
    "### bauen wir uns in diesem Schritt das Gebäude:\n",
    "\n",
    "instanz = Klassenname(\"Auto\",\"Brot\")\n",
    "instanz.funktion_auf_Klasse()\n",
    "\n",
    "instanz2 = Klassenname(\"Nicht Auto\",\"Kein Brot\")\n",
    "instanz2.funktion_auf_Klasse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man sieht also, dass sich Klassen dafür verwenden lassen, Funktionen und Variablen, welche zusammen gehören, kompakt zusammenzuschreiben. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Aufgabe\n",
    "Schreibt eine eigene Klasse, der man interessante Parameter übergibt und die verschieden interessante Funktionen hat, die man ausführen kann!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Das eigentliche Netzwerk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Da Literatur in diesem Bereich zum Großteil auf Englisch ist, verwenden wir\n",
    "hier für den Aufbau der Netzwerkklasse Englisch!\n",
    "\"\"\"\n",
    "\n",
    "class FCN(object):\n",
    "    \"\"\"\n",
    "    Diese Klasse beinhalten sowohl die Gewichte und Architektur eines vollvernetzten Netzwerkes, sowie \n",
    "    Trainingsfunktionen für Backpropagation zur Gradientenbestimmung, Updaten der Gewichte und Validierung während\n",
    "    dem Training.\n",
    "    \"\"\"\n",
    "    def __init__(self, train_imgs, train_targets, layers=[784,30,10], val_imgs=None, val_targets=None):\n",
    "        \"\"\"\n",
    "        Die Liste layers gibt an, wie viele Schichten mit wie vielen \n",
    "        Neuronen man bauen möchte. Die Standardeinstellung hat die \n",
    "        Eingangschicht mit 28*28=784 Neuronen, welche unsere Eingangsdaten \n",
    "        repräsentieren, eine versteckte Schicht und eine Ausgangsschicht mit \n",
    "        10 Neuronen für unsere 10 Klassen.\n",
    "        \"\"\"\n",
    "        self.n_layers    = len(layers)\n",
    "        self.layer_sizes = layers\n",
    "        self.weights     = [np.random.randn(y, x) for x, y in zip(self.layer_sizes[:-1], self.layer_sizes[1:])]\n",
    "        self.biases      = [np.random.randn(y, 1) for y in self.layer_sizes[1:]]\n",
    "        self.train_imgs    = train_imgs\n",
    "        self.train_targets = train_targets\n",
    "        self.val_imgs      = val_imgs\n",
    "        self.val_targets   = val_targets\n",
    "        \n",
    "        self.hot_list = np.eye(10).astype(int)\n",
    "        \n",
    "        \n",
    "    def return_shapes(self):\n",
    "        print(\"Sizes of weights: {}\".format([x.shape for x in self.weights]))\n",
    "        print(\"Sizes of biases: {}\".format([x.shape for x in self.biases]))\n",
    "    \n",
    "    def sigmoid(self,z):\n",
    "        return 1./(1.+np.exp(-z))\n",
    "    \n",
    "    def deriv_sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Ableitung der Sigmoidfunktion.\n",
    "        \"\"\"\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "\n",
    "    def loss(self, output, target):\n",
    "        \"\"\"\n",
    "        Kostenfunktion: quadratische Gewichtung der Differenz.\n",
    "        \"\"\"\n",
    "        return 0.5*1./len(output)*np.sum((output-target)**2)\n",
    "    \n",
    "    def deriv_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        Ableitung der quadratischen Kostenfunktion.\n",
    "        \"\"\"\n",
    "        return (output-target)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Berechne die Ausgabe des Netzwerkes zu einem\n",
    "        Eingangswert x.\n",
    "        \"\"\"\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, x)+b\n",
    "            x = self.sigmoid(z)\n",
    "        return x\n",
    "\n",
    "    def one_hot(self,t):\n",
    "        \"\"\"\n",
    "        Wandelt eine Zahl <t> in eine Liste [...,t,...] um, wobei <...>=<0,...,0> und t der\n",
    "        entsprechend Zahlenposition entspricht.\n",
    "        \"\"\"\n",
    "        return self.hot_list[t]\n",
    "    \n",
    "    def train_gen(self, bs=10):\n",
    "        \"\"\"\n",
    "        Trainingsgenerator, welche bei Bedarf immer einen Trainingsbatch mit Batchsize <bs> ausspuckt.\n",
    "        \"\"\"\n",
    "        seed = np.random.randint(100000)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.train_imgs)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.train_targets)    \n",
    "        \n",
    "        img_sub_paths = [self.train_imgs[i:i+bs] for i in range(0,len(self.train_imgs),bs)]\n",
    "        batch_targets = [self.train_targets[i:i+bs] for i in range(0,len(self.train_targets),bs)]\n",
    "    \n",
    "        for im_sub_path, sub_labs in zip(img_sub_paths, batch_targets):\n",
    "            yield [(np.array(Image.open(im_path)).reshape(-1,1)/255.,self.one_hot(lab).reshape(-1,1)) for im_path,lab in zip(im_sub_path, sub_labs)]\n",
    "                                                                                            \n",
    "\n",
    "    def val_gen(self):\n",
    "        seed = np.random.randint(100000)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.val_imgs)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.val_targets) \n",
    "        \n",
    "        for im_path, lab in zip(self.val_imgs, self.val_targets):\n",
    "            yield np.array(Image.open(im_path)).reshape(-1,1)/255., lab \n",
    "            \n",
    "            \n",
    "    def train(self, epochs, bs, eta):\n",
    "        \"\"\"\n",
    "        Hier trainieren wir das Netzwerk mittels SGDm.\n",
    "            - epochs ist die Anzahl der Iteration, in denen wir das \n",
    "            Netz aus den Daten lernen lassen.\n",
    "            - bs ist die minibatchsize\n",
    "            - eta die Lernrate\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            train_datagen = self.train_gen(bs)\n",
    "            for mini_batch in train_datagen:\n",
    "                self.update(mini_batch, eta)\n",
    "                \n",
    "            if self.val_imgs is not None:\n",
    "                print(\"Epoch {0}: Validation Accuracy: {1}\".format(epoch+1, self.validate()))\n",
    "            else:\n",
    "                print(\"Epoch {}\".format(epoch+1))\n",
    "\n",
    "                      \n",
    "    def update(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Hier updaten wir unsere Gewichte und Offsets\n",
    "        \"\"\"\n",
    "        ### Liste aller Gradientenhalter.\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]                   \n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        \n",
    "        for img, target in mini_batch:\n",
    "            ### Ausführen des Backpropagationalgorithmuses\n",
    "            mini_grad_b, mini_grad_w = self.backpropagate(img, target)\n",
    "            grad_b = [base_gb+dgb for base_gb, dgb in zip(grad_b, mini_grad_b)]\n",
    "            grad_w = [base_gw+dgw for base_gw, dgw in zip(grad_w, mini_grad_w)]\n",
    "                      \n",
    "        ###Updaten der Gewichte und Offsets\n",
    "        self.weights = [w-(eta/len(mini_batch))*gr_w\n",
    "                        for w, gr_w in zip(self.weights, grad_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*gr_b\n",
    "                       for b, gr_b in zip(self.biases, grad_b)]\n",
    "\n",
    "        \n",
    "    def backpropagate(self, img, target):\n",
    "        \"\"\"\n",
    "        Backpropagation Algorithmus\n",
    "        \"\"\"\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        f_value = img #Wert der entsprechenden Aktivierung\n",
    "        f_values = [img] # Liste aller Aktivierungen pro Schicht\n",
    "                      \n",
    "        z_list = [] # Liste aller z-Vektoren pro Schicht\n",
    "                    \n",
    "                      \n",
    "        ### In diesem Abschnitt produzieren wir die Vorhersage des Netzwerkes,\n",
    "        ### um diese dann mit dem wahren Wert vergleichen zu können.\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, f_value)+b\n",
    "            f_value = self.sigmoid(z)\n",
    "            z_list.append(z)\n",
    "            f_values.append(f_value)\n",
    "                      \n",
    "\n",
    "        ### Nun schieben wir den Fehler zurück zu den Gewichten\n",
    "        ### Dabei starten wir mit dem äußersten Layer.\n",
    "        d_loss  = self.deriv_loss(f_values[-1], target)\n",
    "        delta   = d_loss * self.deriv_sigmoid(z_list[-1])\n",
    "\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta, f_values[-2].T)\n",
    "        \n",
    "\n",
    "        ### und jetzt die inneren Schichten\n",
    "        for j in range(2, self.n_layers):\n",
    "            z     = z_list[-j]\n",
    "            d_sig = self.deriv_sigmoid(z)\n",
    "            delta = np.dot(self.weights[-j+1].transpose(), delta) * d_sig\n",
    "            grad_b[-j] = delta\n",
    "            grad_w[-j] = np.dot(delta, f_values[-j-1].transpose())\n",
    "        return (grad_b, grad_w)\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        Validierung laufen lassen.\n",
    "        \"\"\"\n",
    "        val_data   = self.val_gen()\n",
    "        val_result = []\n",
    "        for img, target in val_data:\n",
    "            val_result.append(int(np.argmax(self.forward(img))==target))\n",
    "        \n",
    "        return np.mean(val_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 Trainieren des Nezwerkes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Instanz der Netzwerk-Klasse aufrufen.\n",
    "net = FCN(train_imgs, train_targets, [784,30,10], val_imgs, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Accuracy: 0.5461904761904762\n",
      "Epoch 2: Validation Accuracy: 0.6438095238095238\n",
      "Epoch 3: Validation Accuracy: 0.6780952380952381\n",
      "Epoch 4: Validation Accuracy: 0.7261904761904762\n",
      "Epoch 5: Validation Accuracy: 0.7528571428571429\n",
      "Epoch 6: Validation Accuracy: 0.7614285714285715\n",
      "Epoch 7: Validation Accuracy: 0.7657142857142857\n",
      "Epoch 8: Validation Accuracy: 0.7609523809523809\n",
      "Epoch 9: Validation Accuracy: 0.7695238095238095\n",
      "Epoch 10: Validation Accuracy: 0.7695238095238095\n"
     ]
    }
   ],
   "source": [
    "### Training für 10 Epochen mit MBS 10 und Learningrate 3\n",
    "net.train(10,10,3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualisierung der Klassifizierungsfähigkeit unseres Netzes\n",
    "Hierfür besorgen wir uns ein zufälliges Bild aus dem Validierungsdatensatz und\n",
    "lassen das Netzwerk eine Vorhersage treffen.\n",
    "\"\"\"\n",
    "\n",
    "rand_idx = np.random.randint(len(val_imgs))\n",
    "example_image = np.array(Image.open(val_imgs[rand_idx])).reshape(-1,1)\n",
    "target        = val_targets[rand_idx]\n",
    "pred          = np.argmax(net.forward(example_image))\n",
    "\n",
    "plt.imshow(example_image.reshape(28,28),cmap=\"gray_r\")\n",
    "plt.title(\"True Number: {} | Prediction: {}\".format(target,pred))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6 Aufgaben\n",
    "\n",
    "Natürlich ist nicht alles perfekt - ändert, was ihr ändern könnt und versucht, die höchste Genauigkeit zu erzielen! Tested zuerst an kleinen Datensets, und wenn ihr eine gute Struktur gefunden habt, lasst es auf einem größeren Datensatz laufen!   \n",
    "__Kleine Tipps hierfür:__ Einfluss auf das Konvergenzverhalten haben die Lernrate, mit der die Gewichte upgedatet werden, die Minibatches zum Abschätzen des Gradientens, natürlich die Zahl der Epochen und die Strukture (Tiefe, Anzahl Gewichte) des Netzwerkes! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomathe",
   "language": "python",
   "name": "biomathe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
